{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78cd4ef",
   "metadata": {},
   "source": [
    "# 1: Base Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc156bf7",
   "metadata": {},
   "source": [
    "Very concise summary of findings, see Old Notebooks/ for previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd33d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b397c3f2",
   "metadata": {},
   "source": [
    "Feel free to minimise from Base Data Inspection, I got lost down rabbit holes before I realised this was a machine learning subject not feature engineering. \n",
    "\n",
    "That said I found there was ~100,000 duplicate rows, 0 duplicated columns, 0 constant columns. \n",
    "\n",
    "Chose not to remove the duplicate rows as, I was to *consider that the amount of data for each species in the database available is an indication of its abundance or rarity*\n",
    "\n",
    "Making it a Hierarchal Multiclassification, edge detection problem, given the scarcity of specific wood types, the target to inditify would be less likely to have duplicates then others, resulting in more complicated classification in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde50527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def squish(INS: Path, OUT: Path, CLASS_HEADERS: list[str]):      \n",
    "    files = [\n",
    "\"Data\\BGLBP.parquet\",\n",
    "\"Data\\CSLBP.parquet\",\n",
    "\"Data\\CSSILTP.parquet\",\n",
    "\"Data\\OLBP.parquet\",\n",
    "\"Data\\SCSLBP.parquet\",\n",
    "\"Data\\SILTP.parquet\",\n",
    "\"Data\\Tchebyshev.parquet\",\n",
    "]\n",
    "    dfs = [pl.read_parquet(f) for f in files]\n",
    "    master = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        feat_cols = [c for c in df.columns if c.lower() not in CLASS_HEADERS]\n",
    "        master = master.hstack(df.select(feat_cols))\n",
    "    master.write_parquet(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24325f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "BASE = Path(\"Data\")\n",
    "RAW = Path(\"Data/Raw\")\n",
    "CLASS_HEADERS = [\"family\", \"genus\", \"species\"]\n",
    "for file in RAW.glob(\"*.csv\"):\n",
    "    name = file.stem.split(\"_\")[1]\n",
    "    print(name)\n",
    "    df = pl.read_csv(file, has_header = False)\n",
    "    ncols = len(df.columns) \n",
    "    new_headers = CLASS_HEADERS + [f\"{name}_{i}\" for i in range(1, ncols - len(CLASS_HEADERS) + 1)]\n",
    "    df = df.rename(dict(zip(df.columns, new_headers)))\n",
    "    df.write_parquet(BASE / f\"{name}.parquet\")\n",
    "    print(f\"Rows x Cols: {len(df)} x {ncols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94283cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "MASTER = BASE / \"master.parquet\"\n",
    "squish(BASE, MASTER, CLASS_HEADERS)\n",
    "data = pl.read_parquet(MASTER)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1233670",
   "metadata": {},
   "source": [
    "### Prelim Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURESIZE = (10,6)\n",
    "FONTSIZE = 18\n",
    "newdata = data.copy\n",
    "newdata[\"index\"] = range(1, len(newdata) + 1)\n",
    "np_data = newdata.loc[:,[\"family\", \"genus\", \"species\", \"index\"]].to_numpy()\n",
    "family, genus, species, index = np_data[:,0], np_data[:,1], np_data[:,2], np_data[:, 3]\n",
    "nind = len(newdata[\"index\"])\n",
    "\n",
    "plt.figure(figsize=FIGURESIZE)\n",
    "plt.plot(index, family, \"r\", label=\"Family\")\n",
    "plt.plot(index, genus, \"g\", label=\"Genus\")\n",
    "plt.plot(index, species, \"b\", label=\"Species\")\n",
    "plt.fill_between(index, family, 0, color=\"r\", alpha=0.3)\n",
    "plt.fill_between(index, genus, family, color=\"g\", alpha=0.3)\n",
    "plt.fill_between(index, species, genus, color=\"b\", alpha=0.3)\n",
    "plt.ylabel(\"Instance as Table Index\", fontsize=FONTSIZE)\n",
    "plt.xlabel(\"Index\", fontsize=FONTSIZE)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=FIGURESIZE)\n",
    "plt.plot(index, family, \"r\", label=\"Family\")\n",
    "plt.plot(index, genus, \"g\", label=\"Genus\")\n",
    "plt.plot(index, species, \"b\", label=\"Species\")\n",
    "plt.xlim(0, nind)\n",
    "plt.ylim(-10, 70)\n",
    "plt.fill_between(index, family, 0, color=\"r\", alpha=0.3)\n",
    "plt.fill_between(index, genus, family, color=\"g\", alpha=0.3)\n",
    "plt.fill_between(index, species, genus, color=\"b\", alpha=0.3)\n",
    "plt.ylabel(\"Instance as Table Index\", fontsize=FONTSIZE)\n",
    "plt.xlabel(\"Index\", fontsize=FONTSIZE)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for file in files:\n",
    "    df = pd.read_parquet(file)\n",
    "    print(file)\n",
    "    matrix = df.corr()\n",
    "    classes = [\"family\", \"genus\", \"species\"]\n",
    "    features = [c for c in df.columns if c not in classes]\n",
    "    df.hist(column = features, xlabelsize = 5, ylabelsize = 5, figsize = (10,6), bins = 21)\n",
    "    plt.figure(figsize=(40,40))\n",
    "    sns.heatmap(matrix, annot = True, cmap = \"coolwarm\", fmt = \".2f\", linewidths = 0.5)\n",
    "    plt.title(f\"Correlation Matrix: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f041cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "classes = [\"family\", \"genus\", \"species\"]\n",
    "features = [col for col in df.columns if col not in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6240759",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    min, max = df[c].min(), df[c].max()\n",
    "    exp = set(range(min, max + 1))\n",
    "    present = set(df[c].unique())\n",
    "    missing = sorted(exp - present)\n",
    "    unique = len(df[c].unique())\n",
    "    print(\"_____________________________________________________________________\")\n",
    "    print(c)\n",
    "    print(f\"Uniques {unique}, Minimum: {min}, Maximum: {max}, Missing: {missing}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    dracula = len(df[feature].unique())\n",
    "    if dracula <= 2:\n",
    "        print(f\"{feature}: {dracula}\")\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1074c9e",
   "metadata": {},
   "source": [
    "### Base Data Inspection: Key-Takeaways\n",
    "\n",
    "The labels, whilst being numerical, are just labels, so the line plots might give an understanding of frequency per class pair offers limited insight into the data being presented.\n",
    "\n",
    "DataFrame Dimensions: 293830 x 536 (R x C)\n",
    "\n",
    "Constant Columns: None / All columns had at least 2 Unique values\n",
    "\n",
    "* Class Histograms:\n",
    "    - Families:\n",
    "        - 58 Unique\n",
    "        - 7 familily groups (groups being ~ 3 familiys) saw (less) than 5_000 observations. \n",
    "        - 5 familily groups (groups being ~ 3 familiys) saw (more) than 25_000 observations. \n",
    "        - Missing families [1, 28]\n",
    "    - Genus:\n",
    "        - 191 Unique\n",
    "        - 8 Genus groups (groups being ~ 9 Geni) saw (less) than 10_000 observations. \n",
    "        - 4 Genus groups (groups being ~ 9 Geni) saw (more) than 20_000 observations. \n",
    "        - Missing Genus [30, 62, 70, 110, 141]\n",
    "    - Species:\n",
    "        - 925 Unique\n",
    "        - 9 Species groups (groups being ~ 44 Species) saw (less) than 10_000 observations. \n",
    "        - 6 Species groups (groups being ~ 44 Species) saw (more) than 20_000 observations.\n",
    "        - Missing species [114, 229] \n",
    "\n",
    "Classes & features are both very skewed and  hierarchal (family -> genus -> species) and multi-class (classes are non-binary) in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_vs_expected(df: pl.DataFrame, classes: str):\n",
    "    exp_even = len(df) / df.select(pl.col(classes).n_unique()).item()\n",
    "    LIMITS = {\"family\": 60, \"genus\": 128, \"species\": 927}\n",
    "    full   = pl.DataFrame({classes: range(1, LIMITS[classes] + 1)})\n",
    "    counts = df.group_by(classes).agg(pl.len().alias(\"n\"))\n",
    "    joined = full.join(counts, on=classes, how=\"left\")\n",
    "    missing = joined.filter(pl.col(\"n\").is_null()).get_column(classes).to_list()\n",
    "    dev = (\n",
    "        joined.with_columns((pl.col(\"n\").fill_null(0) - exp_even).alias(\"dev\"))\n",
    "            .sort(classes)\n",
    "    )\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.bar(dev.get_column(classes).to_list(), dev[\"dev\"].to_list(), width=0.9, label=\"Actual − Expected\")\n",
    "    plt.axhline(0, linewidth=1)\n",
    "    handle = Line2D([0],[0], color='none')\n",
    "    label  = f\"Missing {classes}: \" + (\", \".join(map(str, missing)) if missing else \"None\")\n",
    "    plt.legend([handle], [label], loc='center left', bbox_to_anchor=(1.0, 0.5), frameon=False)\n",
    "    N = LIMITS[classes]\n",
    "    step = 5 if N <= 60 else (10 if N <= 150 else 50)\n",
    "    plt.xticks(range(step, N + 1, step))\n",
    "\n",
    "    plt.xlabel(classes.capitalize()); plt.ylabel(\"Actual − Expected\")\n",
    "    plt.title(f\"Deviation per {classes.capitalize()} (expected ≈ {exp_even:.2f}/{classes})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ecc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    data = pl.read_parquet(DATA)\n",
    "    distribution_vs_expected(data, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68feaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d350dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = data.iloc[:, 2].to_numpy()\n",
    "u, inv = np.unique(spec, return_inverse=True)\n",
    "cnt = np.bincount(inv)\n",
    "x1 = np.log1p(cnt)\n",
    "x2 = np.argsort(np.argsort(cnt)) / (len(cnt) - 1 + 1e-9)\n",
    "X_all = np.c_[x1, x2]\n",
    "\n",
    "k = 2\n",
    "TESTSIZE = 0.4\n",
    "\n",
    "km = KMeans(n_clusters=k, n_init=10, random_state=0).fit(x1.reshape(-1,1))\n",
    "order = np.argsort(km.cluster_centers_.ravel())\n",
    "y_all = np.empty_like(km.labels_)\n",
    "for r, c in enumerate(order): y_all[km.labels_ == c] = r\n",
    "X, ins, y, outs = train_test_split(\n",
    "    X_all, y_all, test_size=TESTSIZE, stratify=y_all, random_state=0\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "ins = scaler.transform(ins)\n",
    "y, outs = y, outs\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X, y)\n",
    "xpred   = KNN.predict(X)\n",
    "outpred = KNN.predict(ins)\n",
    "print(\"Train:\", accuracy_score(y, xpred))\n",
    "print(\"Test :\", accuracy_score(outs, outpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b16db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(outs, outpred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
    "y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = KNN.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=10, edgecolors='k')\n",
    "plt.scatter(ins[:,0], ins[:,1], c=outs, s=10, edgecolors='r')\n",
    "plt.title(\"KNN rarity (0=rarest)\")\n",
    "plt.xlabel(\"log1p(count)\") \n",
    "plt.ylabel(\"species frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9230aa0",
   "metadata": {},
   "source": [
    "# Run Here TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb46db8",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "This notebook runs full training pipeline in three stages:\n",
    "\n",
    "- **Stage 1**: SVM rarity gate (GMM/KMeans/KNN frequency split) with ROC/TPR‑FPR plots and threshold sweep.\n",
    "- **Stage 2**: Random‑Forest/Subag/Bagging base models per level (Family & Genus) + a Logistic stacker with meta‑features (entropy, margin, optional gate prob). Includes confusion matrix, ROC and PR plots.\n",
    "- **Stage 3**: Final boosted classifier (AdaBoost, optionally XGBoost if present) on meta‑features from Stage 2 (family/genus stacker probabilities + p_rare).\n",
    "\n",
    "> Adjust the `DATA` path in libraries tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"; os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"; os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import json, time, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Print Overhead\n",
    "def _setup_logging():\n",
    "    level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n",
    "    fmt   = \"[%(asctime)s] %(levelname)s - %(message)s\"\n",
    "    date  = \"%H:%M:%S\"\n",
    "    logging.basicConfig(level=level, format=fmt, datefmt=date)\n",
    "    return logging.getLogger(\"pipeline\")\n",
    "\n",
    "log = _setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 1234\n",
    "DATA   = Path(\"Data/master.parquet\")\n",
    "MODELS = Path(\"Models\"); MODELS.mkdir(exist_ok=True)\n",
    "PLOTS  = Path(\"Plots\");  PLOTS.mkdir(exist_ok=True)\n",
    "PLOTS_SVM = PLOTS / \"SVM\"; PLOTS_SVM.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_S2  = PLOTS / \"Stage2\"; PLOTS_S2.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_S3  = PLOTS / \"Stage3\"; PLOTS_S3.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# what kernels to train (names must match factory keys below)\n",
    "KERNEL_NAMES = [\n",
    "    \"SVMLIN\",\n",
    "    \"SVMRBF_APPROX\",\n",
    "    \"SVMPoly2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy(drop_cols=(\"family\", \"genus\", \"species\")):\n",
    "    t0 = time.perf_counter()\n",
    "    df = pd.read_parquet(DATA)\n",
    "    X = df.drop(columns=list(drop_cols), errors=\"ignore\").to_numpy(dtype=np.float32)\n",
    "    log.info(f\"Loaded data: {DATA.name} | X={X.shape} | cols dropped={drop_cols}\")\n",
    "    log.info(f\"load_xy took {(time.perf_counter()-t0):.2f}s\")\n",
    "    return df, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s1_initial_rare_labels(species, method=\"kmeans\", random_state=RS):\n",
    "    t0 = time.perf_counter()\n",
    "    cnt = Counter(species)\n",
    "    counts = np.fromiter((cnt[s] for s in species), dtype=np.int32, count=len(species))\n",
    "    x = np.log1p(counts).reshape(-1, 1)\n",
    "\n",
    "    if method == \"gmm\":\n",
    "        model = GaussianMixture(n_components=2, max_iter=60, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.means_.ravel())\n",
    "        hard = (model.predict(x) == rare_comp).astype(np.int32)\n",
    "        soft = model.predict_proba(x)[:, rare_comp]\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=2, n_init=1, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.cluster_centers_.ravel())\n",
    "        hard = (model.labels_ == rare_comp).astype(np.int32)\n",
    "        d = np.linalg.norm(x - model.cluster_centers_[rare_comp], axis=1)\n",
    "        soft = (d.max() - d) / (d.max() - d.min() + 1e-9)\n",
    "    elif method == \"knn\":\n",
    "        k = 10\n",
    "        model = KNeighborsRegressor(n_neighbors=k).fit(x, counts)\n",
    "        local_density = model.predict(x)\n",
    "        soft = 1 - (local_density - local_density.min()) / (local_density.max() - local_density.min() + 1e-9)\n",
    "        hard = (soft > np.median(soft)).astype(np.int32)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'gmm','kmeans','knn'\")\n",
    "\n",
    "    log.info(f\"initial_rare_labels[{method}] -> hard={hard.mean():.3f} rare_ratio | soft∈[{soft.min():.3f},{soft.max():.3f}] | {(time.perf_counter()-t0):.2f}s\")\n",
    "    return hard, soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55a568",
   "metadata": {},
   "source": [
    "## Stage 1 — SVM Rarity Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156852e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                            STAGE 1 \n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- s1_helpers: SVM \n",
    "def s1__gamma_from_dim(n_features):  \n",
    "    return 1.0 / max(1, n_features)\n",
    "\n",
    "def s1_SVMLIN(C=1.0):\n",
    "    base = Pipeline([\n",
    "        (\"mm\",  MinMaxScaler()),\n",
    "        (\"lsvc\", LinearSVC(C=C, class_weight=\"balanced\", random_state=RS, dual=\"auto\"))\n",
    "    ])\n",
    "    base.name = \"SVMLIN\"\n",
    "    return base\n",
    "\n",
    "def s1_SVMRBF_APPROX(n_components=512, gamma=1.0, C=1.0):\n",
    "    base = Pipeline([\n",
    "        (\"mm\",  MinMaxScaler()),\n",
    "        (\"rbf\", RBFSampler(gamma=gamma, n_components=n_components, random_state=RS)),\n",
    "        (\"lsvc\", LinearSVC(C=C, class_weight=\"balanced\", random_state=RS, dual=\"auto\"))\n",
    "    ])\n",
    "    base.name = \"SVMRBF_APPROX\"\n",
    "    return base\n",
    "\n",
    "def s1_SVMPoly_APPROX(degree=2, n_components=512, gamma=1.0, coef0=1.0, C=1.0):\n",
    "    base = Pipeline([\n",
    "        (\"mm\",  MinMaxScaler()),\n",
    "        (\"poly\", Nystroem(kernel=\"polynomial\", degree=degree, gamma=gamma, coef0=coef0,\n",
    "                          n_components=n_components, random_state=RS)),\n",
    "        (\"lsvc\", LinearSVC(C=C, class_weight=\"balanced\", random_state=RS, dual=\"auto\"))\n",
    "    ])\n",
    "    base.name = f\"SVMPoly{degree}\"\n",
    "    return base\n",
    "\n",
    "def s1_build_svm(name, n_features, cfg=None):\n",
    "    cfg = cfg or {}\n",
    "    gamma = cfg.get(\"gamma\", s1__gamma_from_dim(n_features))\n",
    "    C     = cfg.get(\"C\", 1.0)\n",
    "    ncomp = cfg.get(\"n_components\", 512)\n",
    "    if name == \"SVMLIN\":\n",
    "        return s1_SVMLIN(C=C)\n",
    "    if name == \"SVMRBF_APPROX\":\n",
    "        return s1_SVMRBF_APPROX(n_components=ncomp, gamma=gamma, C=C)\n",
    "    if name.startswith(\"SVMPoly\"):\n",
    "        d = int(name.replace(\"SVMPoly\",\"\"))\n",
    "        return s1_SVMPoly_APPROX(degree=d, n_components=ncomp, gamma=gamma, coef0=cfg.get(\"coef0\",1.0), C=C)\n",
    "    raise RuntimeError(f\"unknown svm name: {name}\")\n",
    "\n",
    "def s1__svm_tags(methods, kernels, train_size):\n",
    "    pct = int(train_size*100)\n",
    "    return [f\"{m}_{k}_{pct}\" for m in methods for k in kernels]\n",
    "\n",
    "def s1_ready(methods, kernels, train_size):\n",
    "    tags = s1__svm_tags(methods, kernels, train_size)\n",
    "    ok_all = all((MODELS/\"SVM\"/f\"{t}.joblib\").exists() and (MODELS/\"SVM\"/f\"{t}_meta.json\").exists()\n",
    "                 for t in tags)\n",
    "    best_ok = (MODELS/\"SVM\"/\"_best.json\").exists()\n",
    "    return ok_all and best_ok\n",
    "\n",
    "def s1_rebuild_best_from_metas(methods, kernels, train_size):\n",
    "    metas = list((MODELS/\"SVM\").glob(\"*_meta.json\"))\n",
    "    if not metas: return None\n",
    "    best = None\n",
    "    allowed_prefixes = [f\"{m}_{k}_{int(train_size*100)}\" for m in methods for k in kernels]\n",
    "    for m in metas:\n",
    "        tag = m.stem.replace(\"_meta\",\"\")\n",
    "        if tag not in allowed_prefixes:\n",
    "            continue\n",
    "        info = json.load(open(m, \"r\", encoding=\"utf-8\"))\n",
    "        if (best is None) or (info.get(\"auc\",0.0) > best[\"auc\"]):\n",
    "            parts = tag.split(\"_\")\n",
    "            best = {\"tag\": tag, \"method\": parts[0], \"name\": \"_\".join(parts[1:-1]),\n",
    "                    **info}\n",
    "    if best:\n",
    "        json.dump(best, open(MODELS/\"SVM\"/\"_best.json\",\"w\",encoding=\"utf-8\"), indent=2)\n",
    "    return best\n",
    "\n",
    "# --- s1_plots\n",
    "def s1_plot_cm_binary(y_true, y_pred, out_path, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=10)\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels([\"Common(0)\",\"Rare(1)\"]); ax.set_yticklabels([\"Common(0)\",\"Rare(1)\"])\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title); fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "def s1_plot_tpr_fpr(thr, tpr, fpr, out_path, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(thr, tpr, label=\"TPR\", color=\"red\", lw=1.6)\n",
    "    ax.plot(thr, fpr, label=\"FPR\", color=\"blue\", lw=1.6)\n",
    "    ax.plot(thr, (tpr - fpr) - 0.5, label=\"TPR−FPR (offset −0.5)\", color=\"k\", lw=1.6)\n",
    "    ax.invert_xaxis(); ax.set_xlabel(\"Threshold\"); ax.set_ylabel(\"Rate / Offset Rate\")\n",
    "    ax.set_title(title); ax.legend(frameon=False); ax.grid(alpha=0.3)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "    \n",
    "def s1_plot_counts_vs_threshold(y_true, proba, out_png, title, step=0.001, out_csv=None):\n",
    "    \"\"\"\n",
    "    Plots TP, TN, FP, FN counts vs threshold in [0,1].\n",
    "    step=0.001 -> 1001 points. Use 1e-4 if you want ultra-fine (slower).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.int32)\n",
    "    proba  = np.asarray(proba,  dtype=np.float32)\n",
    "\n",
    "    n_steps = int(round(1.0/step)) + 1\n",
    "    thr = np.linspace(0.0, 1.0, n_steps)\n",
    "\n",
    "    TP = np.empty_like(thr, dtype=np.int64)\n",
    "    TN = np.empty_like(thr, dtype=np.int64)\n",
    "    FP = np.empty_like(thr, dtype=np.int64)\n",
    "    FN = np.empty_like(thr, dtype=np.int64)\n",
    "\n",
    "    pos = (y_true == 1)\n",
    "    neg = ~pos\n",
    "\n",
    "    # straight, clear, fast-enough loop\n",
    "    for i, t in enumerate(thr):\n",
    "        pred = (proba >= t)\n",
    "        TP[i] = np.sum(pred & pos)\n",
    "        TN[i] = np.sum(~pred & neg)\n",
    "        FP[i] = np.sum(pred & neg)\n",
    "        FN[i] = np.sum(~pred & pos)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(thr, TP, label=\"TP\")\n",
    "    ax.plot(thr, TN, label=\"TN\")\n",
    "    ax.plot(thr, FP, label=\"FP\")\n",
    "    ax.plot(thr, FN, label=\"FN\")\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(frameon=False)\n",
    "    ax.grid(alpha=0.3)\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    if out_csv:\n",
    "        pd.DataFrame({\"threshold\": thr, \"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN}).to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a25300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- s1_train\n",
    "def s1_train(train_size=0.75, methods=(\"gmm\",\"kmeans\",\"knn\"),\n",
    "             kernels=KERNEL_NAMES, calib_cv=2, max_train=None,\n",
    "             ncomp_rbf=512, ncomp_poly=512, C=1.0):\n",
    "    log.info(f\"[Stage1] Start | train_size={train_size} methods={list(methods)} kernels={list(kernels)}\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    df, X = load_xy()\n",
    "    n_features = X.shape[1]\n",
    "    species = df[\"species\"].to_numpy()\n",
    "    model_dir = MODELS / \"SVM\"; model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best = None\n",
    "    for method in methods:\n",
    "        y_rare, _ = s1_initial_rare_labels(species, method=method)\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y_rare, test_size=(1-train_size), stratify=y_rare, random_state=RS)\n",
    "        log.info(f\"[Stage1] Method={method} | split: train={Xtr.shape[0]} test={Xte.shape[0]}\")\n",
    "\n",
    "        if max_train is not None and Xtr.shape[0] > max_train:\n",
    "            rng = np.random.default_rng(RS)\n",
    "            idx = rng.choice(Xtr.shape[0], size=max_train, replace=False)\n",
    "            Xtr = Xtr[idx]; ytr = ytr[idx]\n",
    "            log.info(f\"[Stage1] Subsampled train -> {Xtr.shape[0]} rows\")\n",
    "\n",
    "        for name in kernels:\n",
    "            cfg = {\"C\": C, \"gamma\": s1__gamma_from_dim(n_features)}\n",
    "            if name == \"SVMRBF_APPROX\":\n",
    "                cfg[\"n_components\"] = ncomp_rbf\n",
    "            if name.startswith(\"SVMPoly\"):\n",
    "                cfg[\"n_components\"] = ncomp_poly\n",
    "            base = s1_build_svm(name, n_features, cfg=cfg)\n",
    "            clf  = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=calib_cv)\n",
    "\n",
    "            tag  = f\"{method}_{name}_{int(train_size*100)}\"\n",
    "            job  = model_dir / f\"{tag}.joblib\"\n",
    "            meta = model_dir / f\"{tag}_meta.json\"\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            clf.fit(Xtr, ytr)\n",
    "            proba = clf.predict_proba(Xte)[:, 1]\n",
    "            fpr, tpr, thr = roc_curve(yte, proba)\n",
    "            auc = roc_auc_score(yte, proba)\n",
    "            t_opt = thr[np.argmax(tpr - fpr)]\n",
    "            yhat = (proba >= t_opt).astype(np.int32)\n",
    "\n",
    "            dump(clf, job)\n",
    "            with open(meta, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"method\": method, \"name\": name, \"auc\": float(auc),\n",
    "                    \"threshold\": float(t_opt),\n",
    "                    \"gamma\": float(cfg[\"gamma\"]), \"C\": float(C),\n",
    "                    \"n_components\": int(cfg.get(\"n_components\", 0)),\n",
    "                    \"degree\": int(name.replace(\"SVMPoly\",\"\")) if name.startswith(\"SVMPoly\") else None\n",
    "                }, f, indent=2)\n",
    "\n",
    "            s1_plot_tpr_fpr(thr, tpr, fpr, PLOTS_SVM / f\"{tag}_tprfpr.png\", f\"{tag} TPR/FPR\")\n",
    "            s1_plot_cm_binary(yte, yhat, PLOTS_SVM / f\"{tag}_cm.png\", title=f\"{tag} Confusion\")\n",
    "            s1_plot_counts_vs_threshold(\n",
    "                y_true=yte,\n",
    "                proba=proba,\n",
    "                out_png=PLOTS_SVM / f\"{tag}_tp_tn_fp_fn.png\",\n",
    "                title=f\"{tag} TP/TN/FP/FN vs Threshold\",\n",
    "                step=0.001,  # set to 1e-4 if you want finer; will be slower\n",
    "                out_csv=PLOTS_SVM / f\"{tag}_tp_tn_fp_fn.csv\"\n",
    "            )\n",
    "\n",
    "            log.info(f\"[Stage1] {tag} | AUC={auc:.3f} thr={t_opt:.3f} | fit+eval {(time.perf_counter()-t1):.2f}s\")\n",
    "            if (best is None) or (auc > best[\"auc\"]):\n",
    "                best = {\"tag\": tag, \"method\": method, \"name\": name,\n",
    "                        \"auc\": float(auc), \"threshold\": float(t_opt),\n",
    "                        \"gamma\": float(cfg[\"gamma\"]), \"C\": float(C),\n",
    "                        \"n_components\": int(cfg.get(\"n_components\", 0)),\n",
    "                        \"degree\": int(name.replace(\"SVMPoly\",\"\")) if name.startswith(\"SVMPoly\") else None}\n",
    "\n",
    "    json.dump(best, open(model_dir / \"_best.json\",\"w\",encoding=\"utf-8\"), indent=2)\n",
    "    log.info(f\"[Stage1] Best={best['tag']} AUC={best['auc']:.3f} thr={best['threshold']:.3f} | total {(time.perf_counter()-t0):.2f}s\")\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- s1_make_oof_prare - OUT OF FOLD PREDICTION / CROSS VAL\n",
    "def s1_make_oof_prare(best, n_splits=3):\n",
    "    out_path = MODELS / \"SVM\" / \"p_rare_oof.npy\"\n",
    "    if out_path.exists():\n",
    "        log.info(f\"[Stage1-OOF] Skip: found {out_path.name}\")\n",
    "        return np.load(out_path)\n",
    "\n",
    "    log.info(f\"[Stage1-OOF] Start | best={best['tag']} | folds={n_splits}\")\n",
    "    df, X = load_xy()\n",
    "    species = df[\"species\"].to_numpy()\n",
    "    y_rare, _ = s1_initial_rare_labels(species, method=best[\"method\"])\n",
    "\n",
    "    cfg = {\n",
    "        \"gamma\": float(best.get(\"gamma\", s1__gamma_from_dim(X.shape[1]))),\n",
    "        \"C\": float(best.get(\"C\", 1.0)),\n",
    "        \"n_components\": int(best.get(\"n_components\", 512)),\n",
    "    }\n",
    "    base = s1_build_svm(best[\"name\"], X.shape[1], cfg=cfg)\n",
    "    clf  = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=2)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RS)\n",
    "    p_rare_oof = cross_val_predict(clf, X, y_rare, cv=cv, method=\"predict_proba\", n_jobs=1)[:, 1]\n",
    "    np.save(out_path, p_rare_oof)\n",
    "    log.info(f\"[Stage1-OOF] Saved OOF p_rare -> {out_path}\")\n",
    "    return p_rare_oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333a17a",
   "metadata": {},
   "source": [
    "## Stage 2 — Family / Genus Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ec507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                            STAGE 2 \n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_ready():\n",
    "    return (MODELS/\"Stage2_family_stacker.joblib\").exists() and \\\n",
    "           (MODELS/\"Stage2_genus_stacker.joblib\").exists() and \\\n",
    "           (MODELS/\"Stage2_family_oof.npy\").exists() and \\\n",
    "           (MODELS/\"Stage2_genus_oof.npy\").exists()\n",
    "\n",
    "def s2__fix_prob(a: np.ndarray) -> np.ndarray:\n",
    "    a = np.asarray(a, dtype=np.float64)\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    s = a.sum(axis=1, keepdims=True)\n",
    "    good = s.squeeze() > 0\n",
    "    a[good] = a[good] / s[good]\n",
    "    return a\n",
    "\n",
    "def _fit_rf_with_fallback(label: str, X, y, base_params: dict) -> RandomForestClassifier:\n",
    "    tries = [\n",
    "        {},  # try base_params first\n",
    "        {\"n_jobs\": 1, \"max_samples\": 0.6, \"max_depth\": 18, \"min_samples_leaf\": 6},\n",
    "        {\"n_jobs\": 1, \"max_samples\": 0.5, \"max_depth\": 16, \"min_samples_leaf\": 8,\n",
    "         \"n_estimators\": max(100, int(base_params[\"n_estimators\"] * 0.75))},\n",
    "        {\"n_jobs\": 1, \"max_samples\": 0.4, \"max_depth\": 14, \"min_samples_leaf\": 10,\n",
    "         \"n_estimators\": max(80, int(base_params[\"n_estimators\"] * 0.5))}\n",
    "    ]\n",
    "\n",
    "    last_err = None\n",
    "    for i, tweak in enumerate(tries, 1):\n",
    "        params = {**base_params, **tweak}\n",
    "        try:\n",
    "            rf = RandomForestClassifier(**params)\n",
    "            t0 = time.perf_counter()\n",
    "            rf.fit(X, y)\n",
    "            dt = time.perf_counter() - t0\n",
    "            oob = getattr(rf, \"oob_score_\", float(\"nan\"))\n",
    "            log.info(\n",
    "                \"[Stage2] %s RF fit try#%d %.2fs | classes=%d | oob_acc=%.4f | params=%s\",\n",
    "                label, i, dt, len(rf.classes_), oob,\n",
    "                {\n",
    "                    \"n_estimators\": params.get(\"n_estimators\"),\n",
    "                    \"n_jobs\": params.get(\"n_jobs\", -1),\n",
    "                    \"max_samples\": params.get(\"max_samples\"),\n",
    "                    \"max_depth\": params.get(\"max_depth\"),\n",
    "                    \"min_samples_leaf\": params.get(\"min_samples_leaf\", 1),\n",
    "                },\n",
    "            )\n",
    "            return rf\n",
    "        except MemoryError as e:\n",
    "            last_err = e\n",
    "            log.warning(f\"[Stage2] {label} RF try#{i} ran out of memory; tightening settings and retrying…\")\n",
    "            import gc; gc.collect()\n",
    "    raise last_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8372b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_train_oob(n_estimators_fam=300, n_estimators_gen=250):\n",
    "    if s2_ready():\n",
    "        log.info(\"[Stage2] Skip: all artefacts found\")\n",
    "        return (np.load(MODELS/\"Stage2_family_oof.npy\"), np.load(MODELS/\"Stage2_genus_oof.npy\"))\n",
    "\n",
    "    log.info(f\"[Stage2] Start | n_estimators_fam={n_estimators_fam} n_estimators_gen={n_estimators_gen}\")\n",
    "    df, X = load_xy()\n",
    "    yF = df[\"family\"].to_numpy()\n",
    "    yG = df[\"genus\"].to_numpy()\n",
    "\n",
    "    fam_base = dict(\n",
    "        n_estimators=n_estimators_fam,\n",
    "        bootstrap=True, oob_score=True,\n",
    "        max_features=\"sqrt\",\n",
    "        max_depth=18,               \n",
    "        min_samples_leaf=4,          \n",
    "        max_samples=0.6,          \n",
    "        n_jobs=2,                  \n",
    "        random_state=RS\n",
    "    )\n",
    "    gen_base = dict(\n",
    "        n_estimators=n_estimators_gen,\n",
    "        bootstrap=True, oob_score=True,\n",
    "        max_features=\"sqrt\",\n",
    "        max_depth=16,              \n",
    "        min_samples_leaf=6,\n",
    "        max_samples=0.5,\n",
    "        n_jobs=2,\n",
    "        random_state=RS\n",
    "    )\n",
    "    fam = _fit_rf_with_fallback(\"Family\", X, yF, fam_base)\n",
    "    import gc; gc.collect()\n",
    "    gen = _fit_rf_with_fallback(\"Genus\",  X, yG, gen_base)\n",
    "\n",
    "    fam_oob = s2__fix_prob(fam.oob_decision_function_)\n",
    "    gen_oob = s2__fix_prob(gen.oob_decision_function_)\n",
    "\n",
    "    dump(fam, MODELS / \"Stage2_family_stacker.joblib\")\n",
    "    dump(gen, MODELS / \"Stage2_genus_stacker.joblib\")\n",
    "    np.save(MODELS / \"Stage2_family_oof.npy\", fam_oob)\n",
    "    np.save(MODELS / \"Stage2_genus_oof.npy\", gen_oob)\n",
    "\n",
    "    log.info(\"[Stage2] Saved models + OOB probs\")\n",
    "    return fam_oob, gen_oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73681319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2__map_labels_to_index(y, classes):\n",
    "    idx = {c:i for i,c in enumerate(classes)}\n",
    "    return np.array([idx[v] for v in y], dtype=np.int32)\n",
    "\n",
    "def s2__plot_confusion_top(name, y_true, y_pred, classes, outdir, topN=20):\n",
    "    cnt = Counter(y_true); top = [c for c,_ in cnt.most_common(topN)]\n",
    "    sel = np.isin(y_true, top)\n",
    "    idx = {c:i for i,c in enumerate(classes)}\n",
    "    lab = [idx[c] for c in top]\n",
    "    cm = confusion_matrix([idx[v] for v in y_true[sel]], [idx[v] for v in y_pred[sel]], labels=lab)\n",
    "    fig, ax = plt.subplots(figsize=(0.45*len(lab)+4, 0.45*len(lab)+4))\n",
    "    im = ax.imshow(cm, aspect=\"auto\")\n",
    "    ax.set_xticks(range(len(lab))); ax.set_yticks(range(len(lab)))\n",
    "    ax.set_xticklabels([str(classes[i]) for i in lab], rotation=90)\n",
    "    ax.set_yticklabels([str(classes[i]) for i in lab])\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(f\"{name} — Confusion (top {topN})\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.savefig(outdir / f\"{name}_confusion_top{topN}.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# ---------- helpers (no seaborn, simple, fast) ----------\n",
    "def _y_idx(y_true, classes):\n",
    "    m = {c:i for i,c in enumerate(classes)}\n",
    "    return np.array([m[v] for v in y_true], dtype=np.int32)\n",
    "\n",
    "def s2_topk_accuracy(y_true, proba, classes, ks=(1,3,5)):\n",
    "    y = _y_idx(y_true, classes)\n",
    "    order = np.argsort(proba, axis=1)[:, ::-1]\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        hit = (order[:, :k] == y[:, None]).any(axis=1)\n",
    "        out[k] = float(hit.mean())\n",
    "    return out\n",
    "\n",
    "def s2_reliability_ece(y_true, proba, classes, bins=20, out_path=None, title=\"Reliability\"):\n",
    "    y = _y_idx(y_true, classes)\n",
    "    conf = proba.max(axis=1)\n",
    "    pred = proba.argmax(axis=1)\n",
    "    correct = (pred == y).astype(np.float64)\n",
    "\n",
    "    edges = np.linspace(0, 1, bins+1)\n",
    "    idx = np.clip(np.digitize(conf, edges) - 1, 0, bins-1)\n",
    "    bin_acc  = np.zeros(bins); bin_conf = np.zeros(bins); bin_cnt = np.zeros(bins)\n",
    "\n",
    "    for b in range(bins):\n",
    "        sel = (idx == b)\n",
    "        n = sel.sum()\n",
    "        if n:\n",
    "            bin_cnt[b]  = n\n",
    "            bin_acc[b]  = correct[sel].mean()\n",
    "            bin_conf[b] = conf[sel].mean()\n",
    "\n",
    "    N = len(conf)\n",
    "    ece = float(np.sum((bin_cnt / N) * np.abs(bin_acc - bin_conf)))\n",
    "\n",
    "    if out_path is not None:\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        ax.plot([0,1],[0,1], ls=\"--\", lw=1, label=\"Perfect\")\n",
    "        ax.plot(bin_conf, bin_acc, marker=\"o\", lw=1.5, label=f\"Model (ECE={ece:.3f})\")\n",
    "        ax.set_xlabel(\"Confidence\"); ax.set_ylabel(\"Empirical accuracy\")\n",
    "        ax.set_title(title); ax.legend(frameon=False); ax.grid(alpha=0.3)\n",
    "        fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "    return ece\n",
    "\n",
    "def s2_confidence_hist(y_true, proba, classes, out_path, title=\"Confidence histogram\"):\n",
    "    y = _y_idx(y_true, classes)\n",
    "    conf = proba.max(axis=1)\n",
    "    pred = proba.argmax(axis=1)\n",
    "    correct = (pred == y)\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.hist(conf[correct], bins=30, alpha=0.6, label=\"Correct\")\n",
    "    ax.hist(conf[~correct], bins=30, alpha=0.6, label=\"Incorrect\")\n",
    "    ax.set_xlabel(\"Max predicted probability\"); ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(title); ax.legend(frameon=False); ax.grid(alpha=0.2)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "def s2_coverage_accuracy(y_true, proba, classes, out_path, title=\"Coverage vs Accuracy\"):\n",
    "    y = _y_idx(y_true, classes)\n",
    "    conf = proba.max(axis=1)\n",
    "    pred = proba.argmax(axis=1)\n",
    "    correct = (pred == y)\n",
    "    thr = np.linspace(0, 1, 101)\n",
    "    cov = []; acc = []\n",
    "    for t in thr:\n",
    "        keep = conf >= t\n",
    "        cov.append(keep.mean())\n",
    "        acc.append(correct[keep].mean() if keep.any() else np.nan)\n",
    "    cov = np.array(cov); acc = np.array(acc)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "    ax1.plot(thr, cov, lw=1.5, label=\"Coverage\")\n",
    "    ax1.set_xlabel(\"Threshold on max prob\"); ax1.set_ylabel(\"Coverage\")\n",
    "    ax1.grid(alpha=0.2)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(thr, acc, lw=1.5, ls=\"--\", label=\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    fig.suptitle(title)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "def s2_class_acc_vs_support(y_true, y_pred, out_path, title=\"Class accuracy vs support\"):\n",
    "    cnt = Counter(y_true)\n",
    "    per_class = sorted(cnt.keys(), key=lambda c: cnt[c], reverse=True)\n",
    "    acc = []\n",
    "    sup = []\n",
    "    for c in per_class:\n",
    "        sel = (y_true == c)\n",
    "        acc.append((y_pred[sel] == c).mean())\n",
    "        sup.append(cnt[c])\n",
    "    acc = np.array(acc); sup = np.array(sup)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    ax.scatter(np.log1p(sup), acc, s=12, alpha=0.7)\n",
    "    ax.set_xlabel(\"log(1 + support)\"); ax.set_ylabel(\"Per-class accuracy\")\n",
    "    ax.set_title(title); ax.grid(alpha=0.2)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "def s2_dump_top_confusions(y_true, y_pred, out_csv, top=40):\n",
    "    from collections import defaultdict\n",
    "    mis = defaultdict(int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if t != p: mis[(t,p)] += 1\n",
    "    rows = sorted(mis.items(), key=lambda kv: kv[1], reverse=True)[:top]\n",
    "    import csv\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"true\",\"pred\",\"count\"])\n",
    "        for (t,p), c in rows: w.writerow([t,p,c])\n",
    "\n",
    "def s2_plot_feature_importance(model, feat_names, out_path, topn=30, title=\"Feature importance (top)\"):\n",
    "    imp = getattr(model, \"feature_importances_\", None)\n",
    "    if imp is None: return\n",
    "    idx = np.argsort(imp)[::-1][:topn]\n",
    "    names = [feat_names[i] for i in idx]\n",
    "    vals  = imp[idx]\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.28*len(names)+1))\n",
    "    ax.barh(range(len(names)), vals[::-1])\n",
    "    ax.set_yticks(range(len(names))); ax.set_yticklabels(names[::-1])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Importance\"); ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# ---------- main Stage-2 evaluation ----------\n",
    "def s2_eval_visuals():\n",
    "    df, _ = load_xy()\n",
    "    yF = df[\"family\"].to_numpy()\n",
    "    yG = df[\"genus\"].to_numpy()\n",
    "\n",
    "    fam = load(MODELS / \"Stage2_family_stacker.joblib\")\n",
    "    gen = load(MODELS / \"Stage2_genus_stacker.joblib\")\n",
    "    fam_oob = np.load(MODELS / \"Stage2_family_oof.npy\")   # normalized earlier\n",
    "    gen_oob = np.load(MODELS / \"Stage2_genus_oof.npy\")\n",
    "\n",
    "    # Predictions (OOB)\n",
    "    fam_pred = fam.classes_[fam_oob.argmax(axis=1)]\n",
    "    gen_pred = gen.classes_[gen_oob.argmax(axis=1)]\n",
    "\n",
    "    # Keep your confusion figures\n",
    "    s2__plot_confusion_top(\"Family\", yF, fam_pred, fam.classes_, PLOTS_S2, topN=40)\n",
    "    s2__plot_confusion_top(\"Genus\",  yG, gen_pred, gen.classes_, PLOTS_S2, topN=80)\n",
    "\n",
    "    # Feature names for importances\n",
    "    feat_names = df.drop(columns=[\"family\",\"genus\",\"species\"], errors=\"ignore\").columns.tolist()\n",
    "\n",
    "    # ---- Family metrics/plots ----\n",
    "    tkF  = s2_topk_accuracy(yF, fam_oob, fam.classes_, ks=(1,3,5))\n",
    "    eceF = s2_reliability_ece(yF, fam_oob, fam.classes_, bins=20,\n",
    "                              out_path=PLOTS_S2 / \"Family_reliability.png\",\n",
    "                              title=\"Family — Reliability\")\n",
    "    s2_confidence_hist(yF, fam_oob, fam.classes_, PLOTS_S2 / \"Family_conf_hist.png\",\n",
    "                       title=\"Family — Confidence (correct vs incorrect)\")\n",
    "    s2_coverage_accuracy(yF, fam_oob, fam.classes_, PLOTS_S2 / \"Family_cov_acc.png\",\n",
    "                         title=\"Family — Coverage vs Accuracy\")\n",
    "    s2_class_acc_vs_support(yF, fam_pred, PLOTS_S2 / \"Family_acc_vs_support.png\",\n",
    "                            title=\"Family — Per-class accuracy vs support\")\n",
    "    s2_dump_top_confusions(yF, fam_pred, PLOTS_S2 / \"Family_top_confusions.csv\", top=40)\n",
    "    s2_plot_feature_importance(fam, feat_names, PLOTS_S2 / \"Family_feature_importance.png\",\n",
    "                               topn=30, title=\"Family — Feature importance\")\n",
    "\n",
    "    log.info(\"[Stage2/Family] TopK=%s | ECE=%.4f\", tkF, eceF)\n",
    "\n",
    "    # ---- Genus metrics/plots ----\n",
    "    tkG  = s2_topk_accuracy(yG, gen_oob, gen.classes_, ks=(1,3,5))\n",
    "    eceG = s2_reliability_ece(yG, gen_oob, gen.classes_, bins=20,\n",
    "                              out_path=PLOTS_S2 / \"Genus_reliability.png\",\n",
    "                              title=\"Genus — Reliability\")\n",
    "    s2_confidence_hist(yG, gen_oob, gen.classes_, PLOTS_S2 / \"Genus_conf_hist.png\",\n",
    "                       title=\"Genus — Confidence (correct vs incorrect)\")\n",
    "    s2_coverage_accuracy(yG, gen_oob, gen.classes_, PLOTS_S2 / \"Genus_cov_acc.png\",\n",
    "                         title=\"Genus — Coverage vs Accuracy\")\n",
    "    s2_class_acc_vs_support(yG, gen_pred, PLOTS_S2 / \"Genus_acc_vs_support.png\",\n",
    "                            title=\"Genus — Per-class accuracy vs support\")\n",
    "    s2_dump_top_confusions(yG, gen_pred, PLOTS_S2 / \"Genus_top_confusions.csv\", top=40)\n",
    "    s2_plot_feature_importance(gen, feat_names, PLOTS_S2 / \"Genus_feature_importance.png\",\n",
    "                               topn=30, title=\"Genus — Feature importance\")\n",
    "\n",
    "    log.info(\"[Stage2/Genus] TopK=%s | ECE=%.4f\", tkG, eceG)\n",
    "    log.info(\"[Stage2] Visuals saved → Plots/Stage2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                            STAGE 3 \n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_ready():\n",
    "    return (MODELS/\"Stage3_Booster.joblib\").exists() and (MODELS/\"Stage3_AdaBoost.joblib\").exists()\n",
    "\n",
    "def s3__build_metaX_y():\n",
    "    df, _ = load_xy()\n",
    "    yS = df[\"species\"].to_numpy()\n",
    "\n",
    "    p_rare = np.load(MODELS / \"SVM\" / \"p_rare_oof.npy\")\n",
    "    fam_oob = np.load(MODELS / \"Stage2_family_oof.npy\")\n",
    "    gen_oob = np.load(MODELS / \"Stage2_genus_oof.npy\")\n",
    "\n",
    "    n = min(len(p_rare), len(fam_oob), len(gen_oob), len(yS))\n",
    "    metaX = np.hstack([p_rare[:n].reshape(-1, 1), fam_oob[:n], gen_oob[:n]])\n",
    "    y = yS[:n]\n",
    "    return metaX, y\n",
    "\n",
    "def s3_plot_confusion_top(y_true, y_pred, out_dir, topN=30):\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    cnt = Counter(y_true); top = [c for c,_ in cnt.most_common(topN)]\n",
    "    sel = np.isin(y_true, top)\n",
    "    idx = {c:i for i,c in enumerate(classes)}\n",
    "    lab = [idx[c] for c in top]\n",
    "    cm = confusion_matrix([idx[v] for v in y_true[sel]], [idx[v] for v in y_pred[sel]], labels=lab)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(0.45*len(lab)+4, 0.45*len(lab)+4))\n",
    "    im = ax.imshow(cm, aspect=\"auto\")\n",
    "    ax.set_xticks(range(len(lab))); ax.set_yticks(range(len(lab)))\n",
    "    ax.set_xticklabels([str(classes[i]) for i in lab], rotation=90)\n",
    "    ax.set_yticklabels([str(classes[i]) for i in lab])\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(\"Species — Confusion (top classes)\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.savefig(out_dir / \"Species_confusion_top.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "    log.info(\"[Stage3] Visual saved → %s\", out_dir / \"Species_confusion_top.png\")\n",
    "    \n",
    "def s3_train():\n",
    "    path = MODELS / \"Stage3_Booster.joblib\"\n",
    "    if path.exists():\n",
    "        log.info(\"[Stage3] Skip: booster already exists\")\n",
    "        return load(path)\n",
    "\n",
    "    log.info(\"[Stage3] Start (HistGB)\")\n",
    "    metaX, y = s3__build_metaX_y()\n",
    "    log.info(f\"[Stage3] metaX={metaX.shape} | y={y.shape}\")\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(metaX, y, test_size=0.2, stratify=y, random_state=RS)\n",
    "\n",
    "    booster = HistGradientBoostingClassifier(learning_rate=0.1, max_iter=100, early_stopping=True, random_state=RS)\n",
    "    t1 = time.perf_counter(); booster.fit(Xtr, ytr); log.info(f\"[Stage3] Booster fit {(time.perf_counter()-t1):.2f}s\")\n",
    "\n",
    "    yhat = booster.predict(Xte)\n",
    "    acc  = accuracy_score(yte, yhat)\n",
    "    log.info(f\"[Stage3] HistGB holdout acc={acc:.4f}\")\n",
    "    dump(booster, path)\n",
    "    log.info(\"[Stage3] Saved %s\", path)\n",
    "\n",
    "    s3_plot_confusion_top(yte, yhat, out_dir=PLOTS_S3, topN=30)\n",
    "    return booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def s3_plot_adaboost_curves(model, X_val, y_val, out_dir):\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    acc1, acc5, ll = [], [], []\n",
    "    classes = model.classes_\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    y_idx = np.fromiter((cls_to_idx[v] for v in y_val), dtype=np.int32, count=len(y_val))\n",
    "\n",
    "    for proba in model.staged_predict_proba(X_val):\n",
    "        pred_top1 = np.argmax(proba, axis=1)\n",
    "        acc1.append((pred_top1 == y_idx).mean())\n",
    "\n",
    "        k = min(5, proba.shape[1])\n",
    "        topk_idx = np.argpartition(proba, -k, axis=1)[:, -k:]\n",
    "        correct_topk = np.any(topk_idx == y_idx[:, None], axis=1)\n",
    "        acc5.append(correct_topk.mean())\n",
    "\n",
    "        # use consistent label order for log_loss\n",
    "        ll.append(log_loss(y_val, proba, labels=classes))\n",
    "\n",
    "    it = np.arange(1, len(acc1) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(it, acc1, label=\"Top-1 Acc\")\n",
    "    ax.plot(it, acc5, label=\"Top-5 Acc\")\n",
    "    ax.set_xlabel(\"Boosting Iteration\"); ax.set_ylabel(\"Accuracy\"); ax.set_title(\"AdaBoost — Accuracy vs Iteration\")\n",
    "    ax.legend(frameon=False); ax.grid(alpha=0.3)\n",
    "    fig.savefig(out_dir / \"AdaBoost_acc_vs_iter.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(it, ll, label=\"LogLoss\")\n",
    "    ax.set_xlabel(\"Boosting Iteration\"); ax.set_ylabel(\"LogLoss\"); ax.set_title(\"AdaBoost — LogLoss vs Iteration\")\n",
    "    ax.legend(frameon=False); ax.grid(alpha=0.3)\n",
    "    fig.savefig(out_dir / \"AdaBoost_logloss_vs_iter.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    pd.DataFrame({\"iter\": it, \"acc_top1\": acc1, \"acc_top5\": acc5, \"logloss\": ll}) \\\n",
    "      .to_csv(out_dir / \"AdaBoost_iter_metrics.csv\", index=False)\n",
    "\n",
    "    log.info(\"[Stage3] Saved AdaBoost iteration curves → %s\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_train_adaboost(n_estimators=400, learning_rate=0.5):\n",
    "    path = (MODELS / \"Stage3_AdaBoost.joblib\")\n",
    "    if path.exists():\n",
    "        log.info(\"[Stage3] Skip: AdaBoost already exists\")\n",
    "        return load(path)\n",
    "\n",
    "    log.info(\"[Stage3] Start (AdaBoost)\")\n",
    "    metaX, y = s3__build_metaX_y()\n",
    "    Xtr, Xte, ytr, yte = train_test_split(metaX, y, test_size=0.2, stratify=y, random_state=RS)\n",
    "\n",
    "    base = DecisionTreeClassifier(max_depth=1, random_state=RS)\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=RS\n",
    "    )\n",
    "    t0 = time.perf_counter(); ada.fit(Xtr, ytr)\n",
    "    log.info(\"[Stage3] AdaBoost fit %.2fs\", time.perf_counter() - t0)\n",
    "\n",
    "    yhat = ada.predict(Xte)\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    log.info(\"[Stage3] AdaBoost holdout acc=%.4f\", acc)\n",
    "\n",
    "    dump(ada, path)\n",
    "    log.info(\"[Stage3] Saved %s\", path)\n",
    "\n",
    "    s3_plot_confusion_top(yte, yhat, out_dir=PLOTS_S3, topN=30)\n",
    "    s3_plot_adaboost_curves(ada, Xte, yte, PLOTS_S3)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d4dc2",
   "metadata": {},
   "source": [
    "## Col Mapping: For testing on Headerless Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path(\"Data/master.parquet\")   \n",
    "MODELS     = Path(\"Models\"); MODELS.mkdir(parents=True, exist_ok=True)\n",
    "COLMAP_OUT = MODELS / \"colmap.json\"\n",
    "DROP_COLS  = (\"family\", \"genus\", \"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef21883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_any(p: Path) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(p)\n",
    "    # default CSV with header row 0; edit if needed\n",
    "    return pd.read_csv(p, sep=\",\", header=0, low_memory=False)\n",
    "\n",
    "def save_colmap():\n",
    "    if not TRAIN_PATH.exists():\n",
    "        raise SystemExit(f\"missing training file: {TRAIN_PATH}\")\n",
    "    df = _read_any(TRAIN_PATH)\n",
    "    feat = [c for c in df.columns if c not in DROP_COLS]\n",
    "    COLMAP_OUT.write_text(\n",
    "        json.dumps({\"features\": feat, \"drop\": list(DROP_COLS), \"count\": len(feat)}, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"[schema] wrote {COLMAP_OUT} | features={len(feat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63529420",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_colmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e5601",
   "metadata": {},
   "source": [
    "# Calling Full Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83799dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 39\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# t_all = time.perf_counter()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# TRAIN_SIZE = 0.75\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# # -------- Stage 3 --------\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# _ = s3_train()                          \u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43ms3_train_adaboost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m         \n\u001b[0;32m     41\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Stage3 ALL] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mperf_counter()\u001b[38;5;241m-\u001b[39mt0)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ALL] Total runtime \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mperf_counter()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt_all)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m, in \u001b[0;36ms3_train_adaboost\u001b[1;34m(n_estimators, learning_rate)\u001b[0m\n\u001b[0;32m     11\u001b[0m base \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRS)\n\u001b[0;32m     12\u001b[0m ada \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(\n\u001b[0;32m     13\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mbase,\n\u001b[0;32m     14\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRS\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter(); \u001b[43mada\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Stage3] AdaBoost fit \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m t0)\n\u001b[0;32m     22\u001b[0m yhat \u001b[38;5;241m=\u001b[39m ada\u001b[38;5;241m.\u001b[39mpredict(Xte)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:167\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    164\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:570\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \n\u001b[0;32m    533\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the discrete SAMME algorithm and return the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 570\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_all = time.perf_counter()\n",
    "\n",
    "TRAIN_SIZE = 0.75\n",
    "METHODS = (\"gmm\", \"kmeans\", \"knn\")\n",
    "KERNELS = KERNEL_NAMES  # [\"SVMLIN\",\"SVMRBF_APPROX\",\"SVMPoly2\"]\n",
    "(PLOTS_S3).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------- Stage 1 --------\n",
    "if s1_ready(METHODS, KERNELS, TRAIN_SIZE):\n",
    "    log.info(\"[Stage1] Skip: all SVM models present\")\n",
    "    best = json.load(open(MODELS / \"SVM\" / \"_best.json\", \"r\", encoding=\"utf-8\"))\n",
    "else:\n",
    "    best = s1_rebuild_best_from_metas(METHODS, KERNELS, TRAIN_SIZE)\n",
    "    if best is None:\n",
    "        best = s1_train(\n",
    "            train_size=TRAIN_SIZE,\n",
    "            methods=METHODS,\n",
    "            kernels=KERNELS,\n",
    "            calib_cv=2,\n",
    "            max_train=None,\n",
    "            ncomp_rbf=512,\n",
    "            ncomp_poly=512,\n",
    "            C=1.0\n",
    "        )\n",
    "    else:\n",
    "        log.info(f\"[Stage1] Rebuilt best from metas -> {best['tag']} (AUC={best['auc']:.3f})\")\n",
    "\n",
    "# S1 OOF p_rare (used later by Stage 3)\n",
    "_ = s1_make_oof_prare(best, n_splits=3)\n",
    "\n",
    "# -------- Stage 2 --------\n",
    "fam_oob, gen_oob = s2_train_oob(n_estimators_fam=300, n_estimators_gen=250)\n",
    "\n",
    "s2_eval_visuals()\n",
    "\n",
    "# -------- Stage 3 --------\n",
    "_ = s3_train()                           \n",
    "# _ = s3_train_adaboost(400, 0.5) - ADABOOST Alterantive        \n",
    "\n",
    "log.info(f\"[Stage3 ALL] {(time.perf_counter()-t0):.2f}s\")\n",
    "    \n",
    "log.info(f\"[ALL] Total runtime {(time.perf_counter() - t_all):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66307f",
   "metadata": {},
   "source": [
    "# STOPPER FOR TRAINING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb4b6a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2c4d3",
   "metadata": {},
   "source": [
    "Set:\n",
    "* INPUT_PATH\n",
    "* OUT_CSV Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282091e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "# Input Path can Handle both Parquet and CSV files but expects a full merged dataset (with removed classes)\n",
    "# Refer to squish() in the preliminary data analysis.\n",
    "\n",
    "INPUT_PATH  = Path(\"Data/master.parquet\")   \n",
    "\n",
    "# Squish method utilises headers, ensure no headers in the new dataset\n",
    "HEADERLESS  = False                 \n",
    "\n",
    "# If CSV\n",
    "CSV_SEP     = \",\"                   \n",
    "OUT_CSV     = Path(\"predictions.csv\")\n",
    "PRINT_HEAD  = 5\n",
    "\n",
    "MODELS = Path(\"Models\")\n",
    "SVM_DIR = MODELS / \"SVM\"\n",
    "COLMAP_PATH = MODELS / \"colmap.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd641dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Helpers\n",
    "def _must(p: Path):\n",
    "    if not p.exists():\n",
    "        raise SystemExit(f\"missing: {p}\")\n",
    "    return p\n",
    "\n",
    "def _read_any(p: Path, headerless: bool, sep: str) -> pd.DataFrame:\n",
    "    if p.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(p)\n",
    "    header = None if headerless else 0\n",
    "    return pd.read_csv(p, sep=sep, header=header, low_memory=False)\n",
    "\n",
    "def _to_float32(a: pd.DataFrame) -> np.ndarray:\n",
    "    return a.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0).to_numpy(np.float32, copy=False)\n",
    "\n",
    "def _align_X(df: pd.DataFrame, feat: list[str], headered: bool, drop_if_present: list[str]) -> np.ndarray:\n",
    "    if headered:\n",
    "        keep = [c for c in df.columns if c not in drop_if_present]\n",
    "        df = df[keep]\n",
    "        return _to_float32(df.reindex(columns=feat, fill_value=0.0))\n",
    "    if df.shape[1] != len(feat):\n",
    "        raise SystemExit(f\"headerless cols={df.shape[1]} but schema expects {len(feat)}\")\n",
    "    return _to_float32(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d820bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_artifacts():\n",
    "    _must(COLMAP_PATH)\n",
    "    cm = json.loads(COLMAP_PATH.read_text(encoding=\"utf-8\"))\n",
    "    feat = cm[\"features\"]\n",
    "    drop = cm.get(\"drop\", [\"family\",\"genus\",\"species\"])\n",
    "\n",
    "    best_meta = json.loads(_must(SVM_DIR / \"_best.json\").read_text(encoding=\"utf-8\"))\n",
    "    svm_gate  = load(_must(SVM_DIR / f\"{best_meta['tag']}.joblib\"))\n",
    "\n",
    "    fam = load(_must(MODELS / \"Stage2_family_stacker.joblib\"))\n",
    "    gen = load(_must(MODELS / \"Stage2_genus_stacker.joblib\"))\n",
    "    booster = load(_must(MODELS / \"Stage3_Booster.joblib\"))\n",
    "    return feat, drop, svm_gate, fam, gen, booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb78bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    feat, drop, svm_gate, fam, gen, booster = _load_artifacts()\n",
    "    df_new = _read_any(_must(INPUT_PATH), headerless=HEADERLESS, sep=CSV_SEP)\n",
    "    X = _align_X(df_new, feat, headered=(not HEADERLESS), drop_if_present=drop)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Stage 1 — rarity gate\n",
    "    if not hasattr(svm_gate, \"predict_proba\"):\n",
    "        raise SystemExit(\"SVM gate must be calibrated with predict_proba\")\n",
    "    p_rare = svm_gate.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Stage 2 — family/genus\n",
    "    fam_proba = fam.predict_proba(X); fam_pred = fam.classes_[fam_proba.argmax(axis=1)]; fam_conf = fam_proba.max(axis=1)\n",
    "    gen_proba = gen.predict_proba(X); gen_pred = gen.classes_[gen_proba.argmax(axis=1)]; gen_conf = gen_proba.max(axis=1)\n",
    "\n",
    "    # Stage 3 — species\n",
    "    metaX = np.hstack([p_rare.reshape(-1,1), fam_proba, gen_proba])\n",
    "    species_pred = booster.predict(metaX)\n",
    "    species_conf = booster.predict_proba(metaX).max(axis=1) if hasattr(booster, \"predict_proba\") else np.ones(len(species_pred), dtype=np.float32)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"p_rare\": p_rare,\n",
    "        \"family_pred\": fam_pred,  \"family_conf\": fam_conf,\n",
    "        \"genus_pred\":  gen_pred,  \"genus_conf\":  gen_conf,\n",
    "        \"species_pred\": species_pred, \"species_conf\": species_conf,\n",
    "    })\n",
    "    out.to_csv(OUT_CSV, index=False)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[predict] rows={len(out)} | wrote {OUT_CSV} | {dt:.2f}s\")\n",
    "    if PRINT_HEAD:\n",
    "        print(out.head(PRINT_HEAD).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48796948",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
