{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f2fbe3",
   "metadata": {},
   "source": [
    "# 1: Base Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c060f",
   "metadata": {},
   "source": [
    "Feel free to minimise from Base Data Inspection, I got lost down rabbit holes before I realised this was a machine learning subject not feature engineering. \n",
    "\n",
    "That said I found there was ~100,000 duplicate rows, 0 duplicated columns, 0 constant columns. \n",
    "\n",
    "Chose not to remove the duplicate rows as, I was to *consider that the amount of data for each species in the database available is an indication of its abundance or rarity*\n",
    "\n",
    "Making it a Hierarchal Multiclassification, edge detection problem, given the scarcity of specific wood types, the target to inditify would be less likely to have duplicates then others, resulting in more complicated classification in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde8ea3",
   "metadata": {},
   "source": [
    "## 1.01 | Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48747626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Package Installs\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "libraries = {\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"polars\": \"polars\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"seaborn\": \"seaborn\",\n",
    "    \"scikit-learn\": \"sklearn\",\n",
    "    \"xgboost\": \"xgboost\",\n",
    "    \"joblib\": \"joblib\",\n",
    "    \"opencv-python\": \"cv2\",\n",
    "    \"cvxopt\": \"cvxopt\",\n",
    "    \"graphviz\": \"graphviz\",\n",
    "    \"pickle\": \"pickle\",\n",
    "    \"logging\": \"logging\",\n",
    "    \"imbalanced-learn\": \"imbalanced-learn\",\n",
    "    \"numba\": \"numba\",\n",
    "    \"gc\": \"gc\",\n",
    "}\n",
    "\n",
    "for pip_name, import_name in libraries.items():\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"{pip_name} Already Installed\")\n",
    "    except ImportError:\n",
    "        print(f\"{pip_name} Installing\")\n",
    "        subprocess.check_call([sys.executable, \"-m\",\"pip\",\"install\",\"--upgrade\", pip_name])\n",
    "print(\"Libraries Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity:\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import numba as noomba\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import log2\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level= logging.INFO,\n",
    "    format=\"%(levelname)s: %(message)s\"\n",
    ")\n",
    "ilog = logging.info\n",
    "\n",
    "FIGSIZE = (10,6)\n",
    "FONTSIZE = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d88b9e",
   "metadata": {},
   "source": [
    "## 1.02 | Definitions / Unfinished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08507a41",
   "metadata": {},
   "source": [
    "#### Helper | Check Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b702239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_delete(file):\n",
    "    if file.exists():\n",
    "        file.unlink()\n",
    "        print(f\"Removed {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf4ba4",
   "metadata": {},
   "source": [
    "#### **Information Gain**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc39cb",
   "metadata": {},
   "source": [
    "#### Helper | Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66185e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    return -np.sum([p * log2(p) for p in probabilities if p > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd009faf",
   "metadata": {},
   "source": [
    "Helper | Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e852ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(df, feature, target):\n",
    "    total_entropy = entropy(df[target].to_numpy())\n",
    "    values, counts = np.unique(df[feature].to_numpy(), return_counts=True)\n",
    "    weighted_entropy = np.sum([\n",
    "        (counts[i] / np.sum(counts)) * entropy(df.loc[df[feature] == v, target].to_numpy())\n",
    "        for i, v in enumerate(values)\n",
    "    ])\n",
    "    return total_entropy - weighted_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e6ba3",
   "metadata": {},
   "source": [
    "#### Helper | Intrinsic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e01053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_information(df, feature):\n",
    "    values, counts = np.unique(df[feature].to_numpy(), return_counts=True)\n",
    "    probabilities = counts / len(df)\n",
    "    return -np.sum([p * log2(p) for p in probabilities if p > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a231998",
   "metadata": {},
   "source": [
    "#### Helper | gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384eaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(df, feature, target):\n",
    "    ig = information_gain(df, feature, target)\n",
    "    ii = intrinsic_information(df, feature)\n",
    "    return ig / ii if ii != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac9aea",
   "metadata": {},
   "source": [
    "#### Helper | Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(dataframe, features):\n",
    "    if isinstance(features, str):\n",
    "        features = [features]\n",
    "    for f in features:\n",
    "        mean = dataframe[f].mean()\n",
    "        median = dataframe[f].median()\n",
    "        std = dataframe[f].std()\n",
    "        print(f\"{f} Statistics: Mean: {mean:.4f}, Median: {median:.4f}, Standard Deviation: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02189ade",
   "metadata": {},
   "source": [
    "Helper | Squish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10124dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squish(INS: Path, OUT: Path, CLASS_HEADERS: list[str]):      \n",
    "    files = [\n",
    "\"Data\\BGLBP.parquet\",\n",
    "\"Data\\CSLBP.parquet\",\n",
    "\"Data\\CSSILTP.parquet\",\n",
    "\"Data\\OLBP.parquet\",\n",
    "\"Data\\SCSLBP.parquet\",\n",
    "\"Data\\SILTP.parquet\",\n",
    "\"Data\\Tchebyshev.parquet\",\n",
    "]\n",
    "    dfs = [pl.read_parquet(f) for f in files]\n",
    "    master = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        feat_cols = [c for c in df.columns if c.lower() not in CLASS_HEADERS]\n",
    "        master = master.hstack(df.select(feat_cols))\n",
    "    master.write_parquet(OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed54d7",
   "metadata": {},
   "source": [
    "Helper | Check File Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_counts(path, filter:str):\n",
    "    return len(list(path.glob(filter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefd671",
   "metadata": {},
   "source": [
    "## 1.03 | Config / Basic Raw_File Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path(\"Data\")\n",
    "RAW = BASE / \"Raw\"\n",
    "\n",
    "directories = [\n",
    "    BASE, \n",
    "    RAW,\n",
    "]\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents = True, exist_ok = True) \n",
    "    print(f\"Directory Check: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29126a6a",
   "metadata": {},
   "source": [
    "File Name Cleaning & Parquet transformation for Local Machine Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = check_file_counts(RAW, \"*.csv\")\n",
    "print(f\"Before: {before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ff877",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_HEADERS = [\"family\", \"genus\", \"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "for file in RAW.glob(\"*.csv\"):\n",
    "    name = file.stem.split(\"_\")[1]\n",
    "    print(name)\n",
    "    df = pl.read_csv(file, has_header = False)\n",
    "    ncols = len(df.columns) \n",
    "    new_headers = CLASS_HEADERS + [f\"{name}_{i}\" for i in range(1, ncols - len(CLASS_HEADERS) + 1)]\n",
    "    df = df.rename(dict(zip(df.columns, new_headers)))\n",
    "    df.write_parquet(BASE / f\"{name}.parquet\")\n",
    "    print(f\"Rows x Cols: {len(df)} x {ncols}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "after = check_file_counts(BASE, \"*.parquet\")\n",
    "print(f\"After: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01ced1",
   "metadata": {},
   "source": [
    "Create master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER = BASE / \"master.parquet\"\n",
    "check_delete(MASTER)\n",
    "squish(BASE, MASTER, CLASS_HEADERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d652b6",
   "metadata": {},
   "source": [
    "Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d77ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_parquet(MASTER)\n",
    "data = data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c37c97",
   "metadata": {},
   "source": [
    "## 1.04 | Base Data Inspection: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9292501",
   "metadata": {},
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62581c1",
   "metadata": {},
   "source": [
    "classes visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(column = classes, xlabelsize = 10, ylabelsize = 10, figsize = (14,14), bins = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78499175",
   "metadata": {},
   "source": [
    "class line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9539dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = data.copy()\n",
    "newdata[\"index\"] = range(1, len(newdata) + 1)\n",
    "np_data = newdata.loc[:,[\"family\", \"genus\", \"species\", \"index\"]].to_numpy()\n",
    "family, genus, species, index = np_data[:,0], np_data[:,1], np_data[:,2], np_data[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURESIZE = (10,6)\n",
    "FONTSIZE = 18\n",
    "nind = len(newdata[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20966b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=FIGURESIZE)\n",
    "plt.plot(index, family, \"r\", label=\"Family\")\n",
    "plt.plot(index, genus, \"g\", label=\"Genus\")\n",
    "plt.plot(index, species, \"b\", label=\"Species\")\n",
    "plt.fill_between(index, family, 0, color=\"r\", alpha=0.3)\n",
    "plt.fill_between(index, genus, family, color=\"g\", alpha=0.3)\n",
    "plt.fill_between(index, species, genus, color=\"b\", alpha=0.3)\n",
    "plt.ylabel(\"Instance as Table Index\", fontsize=FONTSIZE)\n",
    "plt.xlabel(\"Index\", fontsize=FONTSIZE)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=FIGURESIZE)\n",
    "plt.plot(index, family, \"r\", label=\"Family\")\n",
    "plt.plot(index, genus, \"g\", label=\"Genus\")\n",
    "plt.plot(index, species, \"b\", label=\"Species\")\n",
    "plt.xlim(0, nind)\n",
    "plt.ylim(-10, 70)\n",
    "plt.fill_between(index, family, 0, color=\"r\", alpha=0.3)\n",
    "plt.fill_between(index, genus, family, color=\"g\", alpha=0.3)\n",
    "plt.fill_between(index, species, genus, color=\"b\", alpha=0.3)\n",
    "plt.ylabel(\"Instance as Table Index\", fontsize=FONTSIZE)\n",
    "plt.xlabel(\"Index\", fontsize=FONTSIZE)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "\"Data\\BGLBP.parquet\",\n",
    "\"Data\\CSLBP.parquet\",\n",
    "\"Data\\CSSILTP.parquet\",\n",
    "\"Data\\OLBP.parquet\",\n",
    "\"Data\\SCSLBP.parquet\",\n",
    "\"Data\\SILTP.parquet\",\n",
    "\"Data\\Tchebyshev.parquet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafe794",
   "metadata": {},
   "source": [
    "Histograms and Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = pd.read_parquet(file)\n",
    "    print(file)\n",
    "    matrix = df.corr()\n",
    "    classes = [\"family\", \"genus\", \"species\"]\n",
    "    features = [c for c in df.columns if c not in classes]\n",
    "    df.hist(column = features, xlabelsize = 5, ylabelsize = 5, figsize = (10,6), bins = 21)\n",
    "    plt.figure(figsize=(40,40))\n",
    "    sns.heatmap(matrix, annot = True, cmap = \"coolwarm\", fmt = \".2f\", linewidths = 0.5)\n",
    "    plt.title(f\"Correlation Matrix: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed498be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "classes = [\"family\", \"genus\", \"species\"]\n",
    "features = [col for col in df.columns if col not in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    min, max = df[c].min(), df[c].max()\n",
    "    exp = set(range(min, max + 1))\n",
    "    present = set(df[c].unique())\n",
    "    missing = sorted(exp - present)\n",
    "    unique = len(df[c].unique())\n",
    "    print(\"_____________________________________________________________________\")\n",
    "    print(c)\n",
    "    print(f\"Uniques {unique}, Minimum: {min}, Maximum: {max}, Missing: {missing}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    dracula = len(df[feature].unique())\n",
    "    if dracula <= 2:\n",
    "        print(f\"{feature}: {dracula}\")\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape {len(df.columns)} x {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa57d19",
   "metadata": {},
   "source": [
    "## 1.05 | Base Data Inspection: Key-Takeaways\n",
    "\n",
    "The labels, whilst being numerical, are just labels, so the line plots might give an understanding of frequency per class pair offers limited insight into the data being presented.\n",
    "\n",
    "DataFrame Dimensions: 293830 x 536 (R x C)\n",
    "\n",
    "Constant Columns: None / All columns had at least 2 Unique values\n",
    "\n",
    "* Class Histograms:\n",
    "    - Families:\n",
    "        - 58 Unique\n",
    "        - 7 familily groups (groups being ~ 3 familiys) saw (less) than 5_000 observations. \n",
    "        - 5 familily groups (groups being ~ 3 familiys) saw (more) than 25_000 observations. \n",
    "        - Missing families [1, 28]\n",
    "    - Genus:\n",
    "        - 191 Unique\n",
    "        - 8 Genus groups (groups being ~ 9 Geni) saw (less) than 10_000 observations. \n",
    "        - 4 Genus groups (groups being ~ 9 Geni) saw (more) than 20_000 observations. \n",
    "        - Missing Genus [30, 62, 70, 110, 141]\n",
    "    - Species:\n",
    "        - 925 Unique\n",
    "        - 9 Species groups (groups being ~ 44 Species) saw (less) than 10_000 observations. \n",
    "        - 6 Species groups (groups being ~ 44 Species) saw (more) than 20_000 observations.\n",
    "        - Missing species [114, 229] \n",
    "\n",
    "Classes & features are both very skewed and  hierarchal (family -> genus -> species) and multi-class (classes are non-binary) in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddf45f",
   "metadata": {},
   "source": [
    "Using all files and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fa5e1",
   "metadata": {},
   "source": [
    "## Additional Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00479b3e",
   "metadata": {},
   "source": [
    "#### Helper | Distribution vs Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_vs_expected(df: pl.DataFrame, classes: str):\n",
    "    exp_even = len(df) / df.select(pl.col(classes).n_unique()).item()\n",
    "    LIMITS = {\"family\": 60, \"genus\": 128, \"species\": 927}\n",
    "    full   = pl.DataFrame({classes: range(1, LIMITS[classes] + 1)})\n",
    "    counts = df.group_by(classes).agg(pl.len().alias(\"n\"))\n",
    "    joined = full.join(counts, on=classes, how=\"left\")\n",
    "    missing = joined.filter(pl.col(\"n\").is_null()).get_column(classes).to_list()\n",
    "    dev = (\n",
    "        joined.with_columns((pl.col(\"n\").fill_null(0) - exp_even).alias(\"dev\"))\n",
    "            .sort(classes)\n",
    "    )\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.bar(dev.get_column(classes).to_list(), dev[\"dev\"].to_list(), width=0.9, label=\"Actual − Expected\")\n",
    "    plt.axhline(0, linewidth=1)\n",
    "    handle = Line2D([0],[0], color='none')\n",
    "    label  = f\"Missing {classes}: \" + (\", \".join(map(str, missing)) if missing else \"None\")\n",
    "    plt.legend([handle], [label], loc='center left', bbox_to_anchor=(1.0, 0.5), frameon=False)\n",
    "    N = LIMITS[classes]\n",
    "    step = 5 if N <= 60 else (10 if N <= 150 else 50)\n",
    "    plt.xticks(range(step, N + 1, step))\n",
    "\n",
    "    plt.xlabel(classes.capitalize()); plt.ylabel(\"Actual − Expected\")\n",
    "    plt.title(f\"Deviation per {classes.capitalize()} (expected ≈ {exp_even:.2f}/{classes})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d995a90",
   "metadata": {},
   "source": [
    "###  Distribution vs Expected (mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182235d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "DATA = Path(\"Data/master.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"family\", \"genus\", \"species\"]\n",
    "for c in classes:\n",
    "    data = pl.read_parquet(DATA)\n",
    "    distribution_vs_expected(data, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2553f2",
   "metadata": {},
   "source": [
    "The most performative indicators are the classes themselves (no surprise).\n",
    "As the data is hierarchal in structure:\n",
    "\n",
    "Family -> Genus -> Species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6dbdd",
   "metadata": {},
   "source": [
    "For many datasets, samples are not drawn uniformly from the feature space\n",
    "\n",
    "* 0 D: Clusters\n",
    "* 1 D: Line segments - curves\n",
    "* 2 D: Planes / surfaces\n",
    "\n",
    "A ***manifold*** is a term used to descrie a group of samples that locally vary in some dimensions, but not in others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec41faa",
   "metadata": {},
   "source": [
    "## Data Handling / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93068e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Went a different direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea5152",
   "metadata": {},
   "source": [
    "## Entropy's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708443f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2556ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = data.iloc[:, 2].to_numpy()\n",
    "u, inv = np.unique(spec, return_inverse=True)\n",
    "cnt = np.bincount(inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.log1p(cnt)\n",
    "x2 = np.argsort(np.argsort(cnt)) / (len(cnt) - 1 + 1e-9)\n",
    "X_all = np.c_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4873be",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "TESTSIZE = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb840068",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=k, n_init=10, random_state=0).fit(x1.reshape(-1,1))\n",
    "order = np.argsort(km.cluster_centers_.ravel())\n",
    "y_all = np.empty_like(km.labels_)\n",
    "for r, c in enumerate(order): y_all[km.labels_ == c] = r\n",
    "X, ins, y, outs = train_test_split(\n",
    "    X_all, y_all, test_size=TESTSIZE, stratify=y_all, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565257b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "ins = scaler.transform(ins)\n",
    "y, outs = y, outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X, y)\n",
    "xpred   = KNN.predict(X)\n",
    "outpred = KNN.predict(ins)\n",
    "print(\"Train:\", accuracy_score(y, xpred))\n",
    "print(\"Test :\", accuracy_score(outs, outpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(outs, outpred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = conf[0,0] + conf[1,1] + conf[2,2] + conf[3,3]\n",
    "print(check * 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ced88",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b788c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
    "y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = KNN.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=10, edgecolors='k')\n",
    "plt.scatter(ins[:,0], ins[:,1], c=outs, s=10, edgecolors='r')\n",
    "plt.title(\"KNN rarity (0=rarest)\")\n",
    "plt.xlabel(\"log1p(count)\") \n",
    "plt.ylabel(\"species frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aafab",
   "metadata": {},
   "source": [
    "# 1.05 | Base Data Inspection: Key-Takeaways (repeated)\n",
    "\n",
    "The labels, whilst being numerical, are just labels, so the line plots might give an understanding of frequency per class pair offers limited insight into the data being presented.\n",
    "\n",
    "DataFrame Dimensions: 293830 x 536 (R x C)\n",
    "\n",
    "Constant Columns: None / All columns had at least 2 Unique values\n",
    "\n",
    "* Class Histograms:\n",
    "    - Families:\n",
    "        - 58 Unique\n",
    "        - 7 familily groups (groups being ~ 3 familiys) saw (less) than 5_000 observations. \n",
    "        - 5 familily groups (groups being ~ 3 familiys) saw (more) than 25_000 observations. \n",
    "        - Missing families [1, 28]\n",
    "    - Genus:\n",
    "        - 191 Unique\n",
    "        - 8 Genus groups (groups being ~ 9 Geni) saw (less) than 10_000 observations. \n",
    "        - 4 Genus groups (groups being ~ 9 Geni) saw (more) than 20_000 observations. \n",
    "        - Missing Genus [30, 62, 70, 110, 141]\n",
    "    - Species:\n",
    "        - 925 Unique\n",
    "        - 9 Species groups (groups being ~ 44 Species) saw (less) than 10_000 observations. \n",
    "        - 6 Species groups (groups being ~ 44 Species) saw (more) than 20_000 observations.\n",
    "        - Missing species [114, 229] \n",
    "\n",
    "Classes & features are both very skewed and  hierarchal (family -> genus -> species) and multi-class (classes are non-binary) in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fd74c",
   "metadata": {},
   "source": [
    "# 2. | Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530eb64",
   "metadata": {},
   "source": [
    "1. **Boosting models**: AdaBoost and XGBoost.\n",
    "2. **Bagging models**: two Random Forests (one with default settings and one with optimised hyperparameters).\n",
    "3. **Support Vector Machines**: linear and RBF kernels.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ae5ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd80a99",
   "metadata": {},
   "source": [
    "# Method: \n",
    "\n",
    "1. First and foremost, is this wood potentially rare? Or the easier question, is this wood, common?\n",
    "\n",
    "    - Use SVM classifiers to partition the data, not optimised, reiterated settings.\n",
    "    - Exploits hierarchal structure of the data by instead of directly targeting species (1:~900), targets family (1:60)\n",
    "    - Doesnt re-class or add systemic bias.\n",
    "    - Risk the SVM mis-classification on rare wood type as being common, or misclassifying unseen wood types altogether\n",
    "    - SVM classifier to initially distinguish families with rare species (species < 1750 observations)\n",
    "\t- Using both linear and polynomial, exploring weighted? \n",
    "\n",
    "2. Filtering data based on SVM classification, we know the family might be rare, but does what genus is each family have a rare member?\n",
    "\t- Random Forest Bagging, & subagging & hyperparameters.\n",
    "\n",
    "3. We have now established that both the family and genus (predicted) may contain a rare species. to not misspecify the species we look to using;\n",
    "\t- XGboosted decision tree \n",
    "\tor\n",
    "\t- adaboost decision tree stump\n",
    "\tor \n",
    "\t- a weighted average of both and/or all methods to make the final judgement, hoping to capture both rarity, misclassification metrics per sample and class-classification\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3cf28",
   "metadata": {},
   "source": [
    "### Step 1 | Classifying Rarity species rarity with Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f27415",
   "metadata": {},
   "source": [
    "Definetely not up to writing my own custom algorithms at this point, but I definetely understand the limitations of being unfamiliar with the math.\n",
    "\n",
    "Probably could've chosen a better library for SVM's but we're here so just going to run with it. I looked at SVMlib for parallelisation  but found the artifact preservation to be a little puzzling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eebaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, BaggingClassifier,\n",
    "    AdaBoostClassifier, HistGradientBoostingClassifier\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, balanced_accuracy_score, accuracy_score, f1_score, roc_curve, auc, log_loss\n",
    ")\n",
    "\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5763437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb7ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4e732",
   "metadata": {},
   "source": [
    "# LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22fa8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy(drop_cols = (\"family\", \"genus\", \"species\")):\n",
    "    data = pd.read_parquet(DATA)\n",
    "    X = data.drop(columns = list(drop_cols), errors = \"ignore\").to_numpy(dtype=np.float32)\n",
    "    return data, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb8c4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_rare_labels(species, method=\"gmm\", random_state=RS, save_model=True, return_counts=False):\n",
    "    cnt = Counter(species)\n",
    "    counts = np.fromiter((cnt[s] for s in species), dtype=np.int32, count=len(species))\n",
    "    x = np.log1p(counts).reshape(-1, 1)\n",
    "\n",
    "    if method == \"gmm\":\n",
    "        model = GaussianMixture(n_components=2, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.means_.ravel())\n",
    "        hard = (model.predict(x) == rare_comp).astype(np.int32)\n",
    "        soft = model.predict_proba(x)[:, rare_comp]\n",
    "        if save_model: dump(model, MODELS / \"01_species_gmm.joblib\")\n",
    "\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=2, n_init=10, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.cluster_centers_.ravel())\n",
    "        hard = (model.labels_ == rare_comp).astype(np.int32)\n",
    "        d = np.linalg.norm(x - model.cluster_centers_[rare_comp], axis=1)\n",
    "        soft = (d.max() - d) / (d.max() - d.min() + 1e-9)\n",
    "        if save_model: dump(model, MODELS / \"01_species_kmeans.joblib\")\n",
    "\n",
    "    elif method == \"knn\":\n",
    "        k = 10\n",
    "        model = KNeighborsRegressor(n_neighbors=k)\n",
    "        model.fit(x, counts)\n",
    "        local_density = model.predict(x)\n",
    "        soft = 1 - (local_density - local_density.min()) / (local_density.max() - local_density.min() + 1e-9)\n",
    "        hard = (soft > np.median(soft)).astype(np.int32)\n",
    "        if save_model: dump(model, MODELS / \"01_species_knn.joblib\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'gmm', 'kmeans', or 'knn'\")\n",
    "\n",
    "    return (hard, soft, counts) if return_counts else (hard, soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c1c53",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46523e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latest(prefix):\n",
    "    paths = sorted(MODELS.glob(prefix + \"*.joblib\"))\n",
    "    if not paths: raise FileNotFoundError(f\"No saved model with prefix {prefix}\")\n",
    "    return paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5b3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_metrics(outs, proba, threshold, csv_path=None,\n",
    "                   classifier_name=None, method=None, train_size=None):\n",
    "    pred = proba[:, 0] >= threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(outs, pred).ravel()\n",
    "\n",
    "    eps = 1e-12\n",
    "    n = tn + fp + fn + tp\n",
    "\n",
    "    acc  = (tp + tn) / n\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec  = tp / (tp + fn + eps)\n",
    "    spec = tn / (tn + fp + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    fpr  = fp / (fp + tn + eps)\n",
    "    fnr  = fn / (fn + tp + eps)\n",
    "    bal  = 0.5 * (rec + spec)\n",
    "    mcc  = (tp * tn - fp * fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn) + eps)\n",
    "    ll   = log_loss(outs, proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"Classifier\": classifier_name,\n",
    "        \"Method\": method,\n",
    "        \"TrainSize\": train_size,\n",
    "        \"Threshold\": threshold,\n",
    "        \"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn,\n",
    "        \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec,\n",
    "        \"Specificity\": spec, \"F1\": f1,\n",
    "        \"FPR\": fpr, \"FNR\": fnr,\n",
    "        \"Balanced_Acc\": bal, \"MCC\": mcc,\n",
    "        \"LogLoss\": ll\n",
    "    }\n",
    "\n",
    "    if csv_path:\n",
    "        df = pd.DataFrame([metrics])\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False,\n",
    "                  header=not pd.io.common.file_exists(csv_path))\n",
    "        print(f\"Appended → {csv_path}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a009f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(y_true, proba, thr):\n",
    "    p = proba[:,1] if getattr(proba, \"ndim\", 1) == 2 else proba\n",
    "    pred = (p >= thr).astype(int)\n",
    "    cm = confusion_matrix(np.asarray(y_true).astype(int), pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8a135f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "def timed_job(fn, classifier, method, train_size):\n",
    "    cname = classifier.steps[-1][1].__class__.__name__ if hasattr(classifier, \"steps\") else classifier.__class__.__name__\n",
    "    print(f\"▶ Running: {method} – {cname} – {train_size}\")\n",
    "    t0 = time.perf_counter()\n",
    "    result = fn(classifier, method, train_size)\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"✅ Finished: {method} – {cname} – {train_size} in {t1 - t0:.2f}s\")\n",
    "    return result, t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a41565",
   "metadata": {},
   "source": [
    "All credit for **binary plots** belongs to https://www.youtube.com/@machinelearningpractice2089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78561fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_plots(outs, proba, img1path = None, img2path=None, classifier_name=None, method=None, train_size=None):\n",
    "        # --- use positive class probabilities ---\n",
    "    if proba.ndim > 1:\n",
    "        proba_pos = proba[:, 1]\n",
    "    else:\n",
    "        proba_pos = proba\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(outs, proba_pos)\n",
    "    auc_score = roc_auc_score(outs, proba_pos)\n",
    "\n",
    "    # --- TPR/FPR vs threshold ---\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.plot(thresholds, tpr, color=\"b\", label=\"TPR\")\n",
    "    ax.plot(thresholds, fpr, color=\"r\", label=\"FPR\")\n",
    "    ax.plot(thresholds, tpr - fpr, color=\"g\", label=\"TPR−FPR\")\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlabel(\"Threshold\", fontsize=FONTSIZE)\n",
    "    ax.set_ylabel(\"Fraction\", fontsize=FONTSIZE)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    path1 = img1path\n",
    "    fig.savefig(path1, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"roc_plots\")\n",
    "    # --- ROC curve ---\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.plot(fpr, tpr, color=\"b\", lw=2, label=f\"ROC (AUC={auc_score:.3f})\")\n",
    "    ax.plot([0, 1], [0, 1], \"r--\", lw=1)\n",
    "    ax.set_xlabel(\"FPR\", fontsize=FONTSIZE)\n",
    "    ax.set_ylabel(\"TPR\", fontsize=FONTSIZE)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    path2 = img2path\n",
    "    fig.savefig(path2, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7de66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(classifier, X, model_path):\n",
    "    artifact = {\n",
    "        \"model\": classifier,\n",
    "        \"features\": list(X.columns) if hasattr(X, \"columns\") else None\n",
    "    }\n",
    "    dump(artifact, model_path, compress=3)\n",
    "    print(f\"Model saved → {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a26d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    obj = load(model_path)\n",
    "    return obj[\"model\"], obj.get(\"features\"), obj.get(\"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f3ee118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_to_csv(df, path):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            df.to_csv(path, mode=\"a\", index=False, header=not os.path.exists(path))\n",
    "            return\n",
    "        except PermissionError:\n",
    "            time.sleep(0.5)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 | Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59350082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made Plots\n",
      "Made output\n",
      "Made SVM\n",
      "Made clusters\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DATA = Path(\"Data/master.parquet\")\n",
    "RS = RS\n",
    "\n",
    "FIGSIZE = (10,6)\n",
    "FONTSIZE = 10\n",
    "\n",
    "MODELS = Path(\"clusters\")\n",
    "plot_dir = Path(\"Plots\")\n",
    "CSV = Path(\"output\")\n",
    "SVM = Path(\"SVM\")\n",
    "\n",
    "for dir in [plot_dir, CSV, SVM, MODELS]:\n",
    "    dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Made {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d720cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================\n",
    "# - 1) Binary classification - is this a potentially rare species?\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be8bb4",
   "metadata": {},
   "source": [
    "## SVM: Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba87430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXITER = 2_500\n",
    "\n",
    "# --- Linear Function\n",
    "SVMLIN = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)), \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"linear\",\n",
    "        probability=True,\n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "    ))\n",
    "])\n",
    "SVMLIN.name = \"SVMLINEAR\"\n",
    "\n",
    "# --- Radial Basis Function\n",
    "SVMRBF = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)), \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,         \n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "        )\n",
    "    )\n",
    "])\n",
    "SVMRBF.name = \"SVMRBF\"\n",
    "\n",
    "# --- Poly degree 2\n",
    "SVMPoly2 = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)),  \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"poly\",\n",
    "        degree = 2,\n",
    "        probability=True,\n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "    ))\n",
    "])\n",
    "SVMPoly2.name = \"SVMPoly2\"\n",
    "\n",
    "# --- Poly degree 3\n",
    "SVMPoly3 = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)),  \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"poly\",\n",
    "        degree = 3,\n",
    "        probability=True,\n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "    ))\n",
    "])\n",
    "SVMPoly3.name = \"SVMPoly3\"\n",
    "\n",
    "# --- Poly degree 4\n",
    "SVMPoly4 = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)),  \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"poly\",\n",
    "        degree = 4,\n",
    "        probability=True,\n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "    ))\n",
    "])\n",
    "SVMPoly4.name = \"SVMPoly4\"\n",
    "\n",
    "# --- Poly degree 5\n",
    "SVMPoly5 = Pipeline([\n",
    "    (\"mm\", MinMaxScaler()),\n",
    "    (\"lda\", LinearDiscriminantAnalysis(n_components=1)),  \n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"poly\",\n",
    "        degree = 5,\n",
    "        probability=True,\n",
    "        tol=1e-2,\n",
    "        max_iter= MAXITER,\n",
    "        decision_function_shape=\"ovr\"\n",
    "    ))\n",
    "])\n",
    "SVMPoly5.name = \"SVMPoly5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cce4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_TRAIN_LOG(CLASSIFIER, METHOD, TRAINSIZE):\n",
    "    j = getattr(CLASSIFIER, \"name\", None)\n",
    "   \n",
    "    print(f\"Classifier name: {j}\")\n",
    "    \n",
    "    data, X = load_xy()\n",
    "    species = data[\"species\"].to_numpy()\n",
    "    y, p_rare = initial_rare_labels(species, method=METHOD)\n",
    "    X, ins, y, outs = train_test_split(\n",
    "        X, y, test_size=(1 - (0.01 * TRAINSIZE)),\n",
    "        stratify=y, random_state=RS\n",
    "    )\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RS)\n",
    "    _ = cross_val_predict(\n",
    "        CLASSIFIER,\n",
    "        X, y,\n",
    "        cv=cv, \n",
    "        method=\"predict\",\n",
    "        n_jobs = 1,\n",
    "    )\n",
    "\n",
    "    CLASSIFIER.fit(X, y)\n",
    "    proba = CLASSIFIER.predict_proba(ins)\n",
    "    ROC = Path(\"Plots/ROC\")\n",
    "    ROC1 = ROC / f\"{method}_{classifier_name}_{train_size}_roc.png\"\n",
    "    ROC2 = ROC / f\"{method}_{classifier_name}_{train_size}_tprfpr.png\" \n",
    "    \n",
    "    \n",
    "    # save model\n",
    "    mpath = SVM / f\"{METHOD}_{j}_{TRAINSIZE}.joblib\"\n",
    "    save_model(CLASSIFIER, X, mpath)\n",
    "\n",
    "    binary_plots(outs, proba, img1path = ROC2, img2path = ROC1,classifier_name=j, method=METHOD, train_size=TRAINSIZE)\n",
    "    # metrics + plots\n",
    "    binary_metrics(\n",
    "        outs, proba, THRESHOLD,\n",
    "        csv_path= CSV / \"all_metrics.csv\",\n",
    "        classifier_name=j,\n",
    "        method=METHOD,\n",
    "        train_size=TRAINSIZE\n",
    "    )\n",
    "    thresholds = np.linspace(0.35, 0.65, 10001)\n",
    "    TP, TN, FP, FN = [], [], [], []\n",
    "    for thr in thresholds:\n",
    "        tp, tn, fp, fn = threshold(outs, proba, thr)\n",
    "        TP.append(tp); TN.append(tn); FP.append(fp); FN.append(fn)\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    plt.plot(thresholds, TP, \"g\", label=\"TP\")\n",
    "    plt.plot(thresholds, TN, \"r\", label=\"TN\")\n",
    "    plt.plot(thresholds, FP, \"m\", label=\"FP\")\n",
    "    plt.plot(thresholds, FN, \"c\", label=\"FN\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Count\"); plt.legend()\n",
    "    plot_path = plot_dir / f\"{METHOD}_{j}_{TRAINSIZE}_thresholds.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # All ChatGPT this bit\n",
    "    if TRAINSIZE == 75:\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(7, 5))\n",
    "            ax.contourf(xx, yy, Z, alpha=0.3, cmap=\"Pastel1\", levels=25)\n",
    "            ax.contour(xx, yy, Z, colors=\"k\", linewidths=1, levels=[0.5])\n",
    "\n",
    "            # sample to avoid overcrowding\n",
    "            N = 2000  # adjustable\n",
    "            idx = np.random.choice(len(ins2D), size=min(N, len(ins2D)), replace=False)\n",
    "\n",
    "            common = (y_test_common_rare == 0)\n",
    "            rare = (y_test_common_rare == 1)\n",
    "\n",
    "            ax.scatter(ins2D[np.intersect1d(np.where(common)[0], idx), 0],\n",
    "                    ins2D[np.intersect1d(np.where(common)[0], idx), 1],\n",
    "                    s=10, c=\"green\", alpha=0.5, label=\"Common\")\n",
    "\n",
    "            ax.scatter(ins2D[np.intersect1d(np.where(rare)[0], idx), 0],\n",
    "                    ins2D[np.intersect1d(np.where(rare)[0], idx), 1],\n",
    "                    s=10, c=\"blue\", alpha=0.5, label=\"Rare\")\n",
    "\n",
    "            mis_idx = np.intersect1d(np.where(misclassified)[0], idx)\n",
    "            ax.scatter(ins2D[mis_idx, 0],\n",
    "                    ins2D[mis_idx, 1],\n",
    "                    s=40, marker=\"x\", c=\"red\", lw=1.2, label=\"Misclassified\")\n",
    "\n",
    "            ax.legend(frameon=False)\n",
    "            ax.set_title(f\"Decision Boundary – {METHOD} – {j}\")\n",
    "            ax.set_xlabel(\"PC1\")\n",
    "            ax.set_ylabel(\"PC2\")\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Boundary plot skipped ({METHOD} – {j}): {e}\")\n",
    "\n",
    "        \n",
    "    print(f\"✅ Done: {METHOD} – {j} – {TRAINSIZE}\")\n",
    "    return (METHOD, j, TRAINSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa63ea",
   "metadata": {},
   "source": [
    "# Stage 1 | SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier name: SVMLINEAR\n",
      "Classifier name: SVMLINEAR\n",
      "Classifier name: SVMLINEAR\n",
      "Classifier name: SVMRBF\n",
      "Classifier name: SVMRBF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=2500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=2500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=2500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=2500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "FIGSIZE = (10,6)\n",
    "FONTSIZE = 10\n",
    "\n",
    "THRESHOLD = 0.501\n",
    "# TRAINS = [25, 50, 75]\n",
    "TRAINS = [75]\n",
    "methods = [\"gmm\", \"kmeans\", \"knn\"]\n",
    "classifiers = [SVMLIN, SVMRBF, SVMPoly2, SVMPoly3, SVMPoly4, SVMPoly5]\n",
    "\n",
    "\n",
    "tasks = list(product(classifiers, methods, TRAINS))\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(SVM_TRAIN_LOG, *args) for args in tasks]\n",
    "    for f in as_completed(futures):\n",
    "        print(f.result())\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"\\n⏱ total time: {end - start:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02256c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier workings on SVM\n",
    "# for CLASSIFIER in classifiers:\n",
    "#     for METHOD in methods:\n",
    "#         for TRAINSIZE in TRAINS:\n",
    "#             j = CLASSIFIER.__class__.__name__\n",
    "            \n",
    "#             data, X = load_xy()\n",
    "#             species = data[\"species\"].to_numpy()    \n",
    "#             y, p_rare = initial_rare_labels(species, method = METHOD) \n",
    "#             X, ins, y, outs = train_test_split(X, y, test_size=(1-(0.01*TRAINSIZE)), stratify=y, random_state=RS)\n",
    "#             CLASSIFIER.fit(X, y)\n",
    "#             proba = CLASSIFIER.predict_proba(ins)\n",
    "\n",
    "#             mpath = Path(f\"Models/SVM/{METHOD}_{j}_{TRAINSIZE}.joblib\")\n",
    "#             mpath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "#             save_model(CLASSIFIER, X, mpath)\n",
    "\n",
    "#             binary_metrics(\n",
    "#                 outs, proba, THRESHOLD,\n",
    "#                 csv_path=CSV,\n",
    "#                 classifier_name=CLASSIFIER.__class__.__name__,\n",
    "#                 method=METHOD,\n",
    "#                 train_size=TRAINSIZE\n",
    "#             )\n",
    "#             binary_metrics(outs, proba, THRESHOLD)\n",
    "\n",
    "#             thresholds = np.linspace(0.35, 0.65, 10001)\n",
    "#             TP, TN, FP, FN = [], [], [], []\n",
    "\n",
    "#             for thr in thresholds:\n",
    "#                 tp, tn, fp, fn = threshold(outs, proba, thr)\n",
    "#                 TP.append(tp)\n",
    "#                 TN.append(tn)\n",
    "#                 FP.append(fp)\n",
    "#                 FN.append(fn)\n",
    "\n",
    "#             plt.figure(figsize=FIGSIZE)\n",
    "#             plt.plot(thresholds, TP, \"g\", label=\"TP\")\n",
    "#             plt.plot(thresholds, TN, \"r\", label=\"TN\")\n",
    "#             plt.plot(thresholds, FP, \"m\", label=\"FP\")\n",
    "#             plt.plot(thresholds, FN, \"c\", label=\"FN\")\n",
    "#             plt.xlim([0.4, 0.6])\n",
    "#             plt.xlabel(\"Threshold\")\n",
    "#             plt.ylabel(\"Count\")\n",
    "#             plt.legend()\n",
    "#             plot_path = plot_dir / f\"{METHOD}_{j}_{TRAINSIZE:.2f}_thresholds.png\"\n",
    "#             plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "#             plt.close()\n",
    "#             binary_plots(\n",
    "#                 outs, proba,\n",
    "#                 classifier_name=j,\n",
    "#                 method=METHOD,\n",
    "#                 train_size=TRAINSIZE\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gate_metrics = pd.read_csv(\"all_metrics.csv\").sort_values(\"Classifier\")\n",
    "gate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f867592",
   "metadata": {},
   "source": [
    "## Using Kmeans and RBF kernel (3 fold cross validation):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f706c5",
   "metadata": {},
   "source": [
    "![title](Plots/Thresholds/kmeans_SVMRBF_75_thresholds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b62dfe",
   "metadata": {},
   "source": [
    "Do I even need to say much else. Can record list at 0.40 and 0.60 to determine most likely candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb818d",
   "metadata": {},
   "source": [
    "## Using KNN and RBF kernel (3 fold cross validation):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fbb71",
   "metadata": {},
   "source": [
    "![title](Plots/Thresholds/knn_SVMRBF_75_thresholds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69211e59",
   "metadata": {},
   "source": [
    "Best threshold around ~0.515\n",
    "\n",
    "Benefit from the increase in TN detection, and lacked following by FN.\n",
    "\n",
    "Whilst TP starts to plateau and TN's are at high's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055974c",
   "metadata": {},
   "source": [
    "Both used RBF as expected due to the high data volume and RBF margins seperating before refined calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70edf30",
   "metadata": {},
   "source": [
    "# Stage 2 | Random Forest Ensemble for Family/Genus Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebaf1c",
   "metadata": {},
   "source": [
    "Now that rarity has been measured, we can start looking at class-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d52745",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "RS = 1234\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe15ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (10, 6)\n",
    "FONTSIZE = 10\n",
    "plot_RF = Path(\"Plots/RF\"); plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_RF = Path(\"Models/RF\"); model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy(drop_cols = (\"family\", \"genus\", \"species\")):\n",
    "    data = pd.read_parquet(DATA)\n",
    "    X = data.drop(columns = list(drop_cols), errors = \"ignore\").to_numpy(dtype=np.float32)\n",
    "    return data, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca638964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_rare_labels(species, method=\"kmeans\", random_state=RS, save_model=True, return_counts=False):\n",
    "    cnt = Counter(species)\n",
    "    counts = np.fromiter((cnt[s] for s in species), dtype=np.int32, count=len(species))\n",
    "    x = np.log1p(counts).reshape(-1, 1)\n",
    "\n",
    "    if method == \"gmm\":\n",
    "        model = GaussianMixture(n_components=2, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.means_.ravel())\n",
    "        hard = (model.predict(x) == rare_comp).astype(np.int32)\n",
    "        soft = model.predict_proba(x)[:, rare_comp]\n",
    "        if save_model: dump(model, MODELS / \"01_species_gmm.joblib\")\n",
    "\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=2, n_init=10, random_state=random_state).fit(x)\n",
    "        rare_comp = np.argmin(model.cluster_centers_.ravel())\n",
    "        hard = (model.labels_ == rare_comp).astype(np.int32)\n",
    "        d = np.linalg.norm(x - model.cluster_centers_[rare_comp], axis=1)\n",
    "        soft = (d.max() - d) / (d.max() - d.min() + 1e-9)\n",
    "        if save_model: dump(model, MODELS / \"01_species_kmeans.joblib\")\n",
    "\n",
    "    elif method == \"knn\":\n",
    "        k = 10\n",
    "        model = KNeighborsRegressor(n_neighbors=k)\n",
    "        model.fit(x, counts)\n",
    "        local_density = model.predict(x)\n",
    "        soft = 1 - (local_density - local_density.min()) / (local_density.max() - local_density.min() + 1e-9)\n",
    "        hard = (soft > np.median(soft)).astype(np.int32)\n",
    "        if save_model: dump(model, MODELS / \"01_species_knn.joblib\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'gmm', 'kmeans', or 'knn'\")\n",
    "\n",
    "    return (hard, soft, counts) if return_counts else (hard, soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_row(p):\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    return -(p * np.log(p)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2542c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_margin(p):\n",
    "    s = np.sort(p, axis=1)[:, ::-1]\n",
    "    return s[:, 0] - s[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_vote(preds):\n",
    "    preds = np.stack(preds, axis=1)\n",
    "    return np.array([np.bincount(row).argmax() for row in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_vote(prob_list):\n",
    "    P = np.mean(prob_list, axis=0)\n",
    "    return P, P.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0655129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_bound(preds, y_true):\n",
    "    preds = np.stack(preds, axis=1)\n",
    "    ok = (preds == y_true[:, None]).any(axis=1)\n",
    "    return ok.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_true, y_pred, title, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(cm, cmap=\"YlGnBu\", annot=False, cbar=True)\n",
    "    plt.title(f\"{title}\\nNormalized Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score, n_classes, title):\n",
    "    y_true_bin = pd.get_dummies(y_true)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.iloc[:, i], y_score[:, i])\n",
    "        auc = roc_auc_score(y_true_bin.iloc[:, i], y_score[:, i])\n",
    "        plt.plot(fpr, tpr, lw=1, label=f\"Class {i} (AUC={auc:.2f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\",c=\"grey\")\n",
    "    plt.title(f\"ROC Curves – {title}\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr(y_true, y_score, n_classes, title):\n",
    "    y_true_bin = pd.get_dummies(y_true)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    for i in range(n_classes):\n",
    "        pr, rc, _ = precision_recall_curve(y_true_bin.iloc[:, i], y_score[:, i])\n",
    "        plt.plot(rc, pr, lw=1, label=f\"Class {i}\")\n",
    "    plt.title(f\"Precision–Recall – {title}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data, X = load_xy()\n",
    "species = data[\"species\"].to_numpy()\n",
    "y_family = data[\"family\"].to_numpy()\n",
    "y_genus  = data[\"genus\"].to_numpy()\n",
    "_, p_rare = initial_rare_labels(species, method=\"kmeans\")\n",
    "\n",
    "# Split consistent with Stage 1\n",
    "X, ins, yF, outsF = train_test_split(X, y_family, test_size=0.2, stratify=y_family, random_state=RS)\n",
    "_, insG, yG, outsG = train_test_split(X, y_genus,  test_size=0.2, stratify=y_genus,  random_state=RS)\n",
    "cv = StratifiedKFold(n_splits=K, shuffle=True, random_state=RS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_family = [\n",
    "    (\"RF\",  RandomForestClassifier(n_estimators=300, bootstrap=True,  n_jobs=-1, random_state=RS)),\n",
    "    (\"Sub\", RandomForestClassifier(n_estimators=300, bootstrap=False, n_jobs=-1, random_state=RS)),\n",
    "    (\"Bag\", BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RS),\n",
    "                              n_estimators=200, bootstrap=True, n_jobs=-1, random_state=RS)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b20c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_genus = [\n",
    "    (\"RF\",  RandomForestClassifier(n_estimators=400, bootstrap=True,  n_jobs=-1, random_state=RS)),\n",
    "    (\"Sub\", RandomForestClassifier(n_estimators=400, bootstrap=False, n_jobs=-1, random_state=RS)),\n",
    "    (\"Bag\", BaggingClassifier(estimator=DecisionTreeClassifier(random_state=RS),\n",
    "                              n_estimators=300, bootstrap=True, n_jobs=-1, random_state=RS)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_ensemble(X, y, ins, outs, base_models, label=\"family\"):\n",
    "    n_classes = len(np.unique(y))\n",
    "    oof_list, preds_list, prob_list = [], [], []\n",
    "\n",
    "    print(f\"\\nTraining base models for {label.upper()} ...\")\n",
    "    for name, est in base_models:\n",
    "        oof = cross_val_predict(est, X, y, cv=cv, method=\"predict_proba\", n_jobs=-1)\n",
    "        oof_list.append(oof)\n",
    "        est.fit(X, y)\n",
    "        p = est.predict(ins)\n",
    "        P = est.predict_proba(ins)\n",
    "        preds_list.append(p); prob_list.append(P)\n",
    "        print(f\"  {name:<4} acc = {accuracy_score(outs, p):.4f}\")\n",
    "\n",
    "    # --- stack meta features ---\n",
    "    stack_train = np.hstack(oof_list)\n",
    "    stack_test  = np.hstack(prob_list)\n",
    "    stack_train = np.hstack([stack_train,\n",
    "                             entropy_row(np.mean(oof_list, axis=0)).reshape(-1,1),\n",
    "                             top_margin(np.mean(oof_list, axis=0)).reshape(-1,1)])\n",
    "    stack_test  = np.hstack([stack_test,\n",
    "                             entropy_row(np.mean(prob_list, axis=0)).reshape(-1,1),\n",
    "                             top_margin(np.mean(prob_list, axis=0)).reshape(-1,1),\n",
    "                             p_rare[:len(ins)].reshape(-1,1)])  # gate prob as meta feature\n",
    "\n",
    "    stacker = LogisticRegression(max_iter=200, n_jobs=-1, random_state=RS)\n",
    "    stacker.fit(stack_train, y)\n",
    "    y_pred_stack = stacker.predict(ins)\n",
    "    y_proba_stack = stacker.predict_proba(ins)\n",
    "\n",
    "    # --- metrics ---\n",
    "    acc_vote = accuracy_score(outs, hard_vote(preds_list))\n",
    "    acc_soft = accuracy_score(outs, soft_vote(prob_list)[1])\n",
    "    acc_stack = accuracy_score(outs, y_pred_stack)\n",
    "    acc_oracle = oracle_bound(preds_list, outs)\n",
    "\n",
    "    print(f\"\\nResults – {label.upper()}\")\n",
    "    print(f\"  Hard vote  : {acc_vote:.4f}\")\n",
    "    print(f\"  Soft vote  : {acc_soft:.4f}\")\n",
    "    print(f\"  Stacker    : {acc_stack:.4f}\")\n",
    "    print(f\"  Oracle     : {acc_oracle:.4f}\\n\")\n",
    "    print(classification_report(outs, y_pred_stack, digits=3))\n",
    "\n",
    "    # --- plots ---\n",
    "    plot_confusion(outs, y_pred_stack, f\"{label.upper()} – Stacker\")\n",
    "    plot_roc(outs, y_proba_stack, n_classes, f\"{label.upper()} – Stacker\")\n",
    "    plot_pr(outs, y_proba_stack, n_classes, f\"{label.upper()} – Stacker\")\n",
    "\n",
    "    return stacker, y_pred_stack, y_proba_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ae250",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "fam_stacker, fam_pred, fam_proba = train_meta_ensemble(X, yF, ins, outsF, base_family, label=\"family\")\n",
    "gen_stacker, gen_pred, gen_proba = train_meta_ensemble(X, yG, insG, outsG, base_genus, label=\"genus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mapping_family_to_genus(family, genus):\n",
    "    mapping = {}\n",
    "    for f, g in zip(family, genus):\n",
    "        mapping.setdefault(int(f), set()).add(int(g))\n",
    "    return mapping\n",
    "\n",
    "def enforce_hierarchy(pred_family, proba_genus, mapping):\n",
    "    fixed = proba_genus.copy()\n",
    "    for i, f in enumerate(pred_family):\n",
    "        allowed = mapping.get(int(f), None)\n",
    "        if allowed is None: continue\n",
    "        mask = np.ones(fixed.shape[1], dtype=bool)\n",
    "        mask[list(allowed)] = False\n",
    "        fixed[i, mask] = 0\n",
    "        s = fixed[i].sum()\n",
    "        if s > 0: fixed[i] /= s\n",
    "    return fixed, fixed.argmax(axis=1)\n",
    "\n",
    "fam2gen = build_mapping_family_to_genus(y_family, y_genus)\n",
    "gen_fixed_proba, gen_fixed_pred = enforce_hierarchy(fam_pred, gen_proba, fam2gen)\n",
    "\n",
    "print(f\"\\n[Hierarchy] genus corrections {(gen_fixed_pred != gen_pred).mean():.3f} of samples changed.\")\n",
    "print(f\"[Hierarchy] corrected genus acc = {accuracy_score(outsG, gen_fixed_pred):.4f}\")\n",
    "\n",
    "plot_confusion(outsG, gen_fixed_pred, \"GENUS – Hierarchy-Corrected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577fcd0",
   "metadata": {},
   "source": [
    "# Stage 3 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01505f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc7a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bf43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f292c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f40a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06895ade",
   "metadata": {},
   "source": [
    "# Stage 3: Boosting Classifier for Final Species Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963e4ed",
   "metadata": {},
   "source": [
    "Purpose: The final stage predicts the exact species. This is the most fine-grained classification, and we use a boosting ensemble to maximize accuracy. Boosting works by sequentially training “weak” learners and focusing on mistakes of the previous ones\n",
    "scikit-learn.org\n",
    ". The two common options are AdaBoost (Adaptive Boosting) and Gradient Boosting. Scikit-learn provides AdaBoostClassifier (which by default uses shallow decision trees as base estimators) and the more recent HistGradientBoostingClassifier – a fast implementation of gradient-boosted trees that is great for tabular data\n",
    "scikit-learn.org\n",
    ". We can choose either; here we’ll demonstrate with AdaBoost (for multi-class, scikit-learn uses the SAMME.R algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import dump\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "FIGSIZE = (9,6)\n",
    "RS = 42\n",
    "plot_dir = Path(\"Plots/RF\")\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir = Path(\"Models/RF\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier  # or HistGradientBoostingClassifier\n",
    "\n",
    "species_clf = AdaBoostClassifier(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "                                 random_state=42)\n",
    "species_clf.fit(X_train_species, y_train_species)  # y_train_species has the final species labels\n",
    "dump(species_clf, \"models/species_boost_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_species = species_clf.predict(X_test_species)\n",
    "cm_species = confusion_matrix(y_test_species, y_pred_species, labels=species_clf.classes_)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_species,\n",
    "                      display_labels=species_clf.classes_).plot(cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
    "plt.title(\"Species Classifier Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_plots(outs, proba, classifier_name=None, method=None, train_size=None):\n",
    "\n",
    "#     fpr,tpr, thresholds = roc_curve(outs, proba[:,0])\n",
    "#         # TPR/FPR\n",
    "#     print(tpr-fpr)\n",
    "#     fig,ax = plt.subplots(figsize=FIGSIZE)\n",
    "#     ax.plot(thresholds, tpr, color = \"b\")\n",
    "#     ax.plot(thresholds, fpr, color = \"r\")\n",
    "#     ax.plot(thresholds, (tpr-fpr)-0.1, color = \"g\")\n",
    "#     ax.invert_xaxis()\n",
    "#     ax.set_xlabel(\"threshold\", fontsize=FONTSIZE)\n",
    "#     ax.set_ylabel(\"fraction\", fontsize=FONTSIZE)\n",
    "#     ax.legend([\"TPR\", \"FPR\", \"distance\"])\n",
    "#     plot_dir = Path(\"Plots\")\n",
    "#     thresh_path = plot_dir / f\"{method}_{classifier_name}_{train_size}_tprfpr.png\"\n",
    "#     fig.savefig(thresh_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#     # ROC plot\n",
    "#     fig,ax = plt.subplots(figsize=FIGSIZE)\n",
    "#     ax.plot(fpr,tpr, color=\"b\")\n",
    "#     ax.plot([0,1], [0,1], \"r--\")\n",
    "#     ax.set_xlabel(\"FPR\", fontsize=FONTSIZE)\n",
    "#     ax.set_ylabel(\"TPR\", fontsize=FONTSIZE)\n",
    "#     ax.set_aspect(\"equal\", \"box\")\n",
    "#     roc_path = plot_dir / f\"{method}_{classifier_name}_{train_size}_roc.png\"\n",
    "#     fig.savefig(roc_path, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c872f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc311e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532d545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fef0b955",
   "metadata": {},
   "source": [
    "# Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31765d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8185855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66804bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a5c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b349810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c9360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbf49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dc041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af68ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf87d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23598fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cd4ebc",
   "metadata": {},
   "source": [
    "# Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19470b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Gate\n",
    "# MODEL = \"models/rare_gate_supervised.joblib\"\n",
    "# def predict_rare(X_new, model_path=MODEL):\n",
    "#     m = joblib.load(model_path)\n",
    "#     pred = m.predict(X_new.astype(np.float32))       # 1=rare, 0=common\n",
    "#     return pred\n",
    "# X_new = ...  # numpy array [n_samples, n_features]\n",
    "# rare_flags = predict_rare(X_new)  # 1=rare, 0=common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50b861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a94695c",
   "metadata": {},
   "source": [
    "Should be madatory watching\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/@machinelearningpractice2089"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
